---
bibliography: [references.bib]
---

# Introduction to Computational Modelling
Modelling aims to describe elements of a system of interest, their states, and their interactions with other elements. A model should be detailed and precise enough to simulate the system's behavior on a computer. Living cells are complex, with biochemical processes organized across various compartments like the extracellular space, cytosol, plasma membrane, and organelles. These processes are multiscale, involving small-scale molecular interactions that lead to emergent, larger-scale cellular behaviors. Understanding these dynamic, multiscale processes requires appropriate mathematical tools to analyze and control the system's behavior, which can help understanding life and disease.

An important aim of modelling is to make clear the current state of knowledge regarding a particular system, by being precise about the elements involved and the interactions between them. Doing this can be an effective way to highlight gaps in understanding. Our understanding of experimental observations in any system can be measured by how closely the simulations we create mimic the system's real behavior. Behaviours of computer executable models are at first compared and validated with experimental values. If at this stage inconsistency is found, it means that the assumptions, that represent our knowledge on the system, are at best incomplete, or that the interpretation of the experimental data is wrong. Models that  survive this initial validation can then be used to make predictions to be tested by experiments, as well to explore configurations of the system that are not easy to investigate by _in vitro_ or _in vivo_ experiments.  Creating predictive models can give opportunities for unprecedented control over the system. Modelling can provide valuable insights into the workings and general principles of organization of biological systems.

Using computational methods for studying biological mechanisms can offer many advantages, including considerable time and cost-efficient savings. Using computer models allows us to study the specific molecules in question, for example, with the ability of testing minimal requirements of very complex systems by just considering the molecules immediately influencing the phenomenon that is being studied. Of course this comes with the caveat that the whole system dynamics cannot be observed. Nevertheless, useful predictions can come out of these models to combine with wet lab research and a combination of both can be useful for creating predictions about biological phenomena. 

This project looks at biology at the level of protein-protein interactions and dynamics. It takes the view that looking at these dynamics is necessary if we want to understand emergent properties of these interactions. Therefore, modelling, simulation, and analysis of simulation outcomes are perfectly positioned for integration into the experimental cycle of cell/molecular biology. Although _in vitro_ and _in vivo_ experiments might still be needed to advance our understanding of biological processes, conducting in silico, or computer-simulated experiments can help guide the wet-lab process by narrowing the experimental search space. This in turn can mean a reduction of repeated wet-lab experiments, meaning reduced suffering of non-human animals, in accordance with the “Replacement” R of the 3Rs framework @tannenbaum2015Russell. 

look for inspo in pharris intro paper 

## How do we model biochemical systems networks?

Biological and chemical systems can be described by networks of chemical reactions; in other words, the cascades of interactions between CaMKII with other molecules can be understood as a network of chemical reactions. And computational models of these reaction networks can be used to elucidate their dynamics. 

Reaction-based models are formalized as sets of reactions that describe the given system in terms of mechanistic interactions between the species of interest. This is, biochemical networks are a set of chemical species that can be converted into each other through chemical reactions. The focus of biochemical network models is usually on the levels of the chemical species and this usually requires explicit mathematical expressions for the velocity at which the reactions proceed. Biological systems can be simulated in different ways using different algorithms depending on the assumptions made about the underlying kinetics. Once the kinetics have been specified, these systems can be used directly to construct full dynamic simulations of the system behaviour on a computer.

## Computational models of CaMKII and NMDARs

Numerous computational models of CaMKII dynamics have been developed over the past decade to look at different interactions of CaMKII and its substrates or binding partners at varying levels of detail [REFERENCES]. Many of these models center on the binding of calcium to calmodulin and the subsequent formation of the CaM-CaMKII complex [REFERENCES], but not on the CaMKII/NMDAR complex therefore making this research novel in its field. The research carried out in this PhD project follows recent studies [Pharris, Ordyan, Bartol] that have looked at CaMKII as a dodecamer to study its autophosphorylation and spatiotemporal regulation in poststynaptic dendrites using BioNetGen and MCell. Pharris look at this, Ordyan looks at this, Bartol looks at this. This is what I look at.

There is a plethora of computational models with a focus around CaMKII that use software such as NEURON [@maki-marttunen2020Unified], Smoldyn [@khan2011Sequestration], Cellular Dynamic Simulator [@byrne2011Impacts; @byrne2010Cellular], and more [REFERENCES]. While these studies provide a good foundation for understanding CaMKII dynamics, the models either focus solely on temporal dynamics without considering geometry [@maki-marttunen2020Unified], or explore both spatial and temporal dynamics but do not model CaMKII as a multimeric molecule [Pharris, CDS paper]. Some models do account for CaMKII as a multimeric dodecamer but do not focus on its interactions with NMDARs. To my knowledge, this research is novel in the field, as it incorporates all these aspects: modeling CaMKII as a dodecamer, examining spatiotemporal dynamics, and focusing specifically on the CaMKII/NMDAR complex.

Incorporating spatial, temporal, and multimeric aspects is crucial because these dimensions fundamentally shape the behavior of molecular complexes like like the CaMKII/NMDAR interaction. Spatial dynamics can reveal how the molecules interact within different cellular regions, in fact CaMKII phosphorylation levels, as well as its functions have been shown to differ depending on its subcellular localization [@davies2007ACaMKII]. Temporal dynamics are important to understand how CaMKII/NMDAR interactions evolve over time; which is a matter yet to be fully understood, such as: how long does the CaMKII/NMDAR complex last during and/or after LTP?. 

_Needs to go somewhere else not sure where:_ More recently, research is shifting from the long-standing idea that CaMKII behaves like a bistable switch to a new understanding involving a process known as "kinetic proofreading" that enhances the specificity and fidelity of CaMKII-mediated signaling. Bartol et al., 2024 propose that for LTP to occur, the autophosphorylation of CaMKII subunits does not need to reach the high, irreversible steady-state associated with the bistable switch theory. Instead, they propose that the integration of signals at the synapse must lead to a temporary surge in autophosphorylation that is sufficient to initiate one or more downstream processes, which are essentially irreversible, ultimately resulting in synaptic potentiation. This proposed mechanism aligns well with the hypothesis that the CaMKII/NMDAR complex at the PSD plays a critical role in stabilizing CaMKII in its autophosphorylated state, thereby facilitating synaptic potentiation. 

----

## Why use Computational Modelling to study biological systems?

Modelling aims to describe elements of a system of interest, their states, and their interactions with other elements. A model should be detailed and precise enough to simulate the system's behavior on a computer. Living cells are complex, with biochemical processes organized across various compartments like the extracellular space, cytosol, plasma membrane, and organelles. These processes are multiscale, involving small-scale molecular interactions that lead to emergent, larger-scale cellular behaviors. Understanding these dynamic, multiscale processes requires appropriate mathematical tools to analyze and control the system's behavior, which can help understanding life and disease.

An important concern with these investigations, which view health and disease in a more "detached-from-patient" way, is whether the results could reinforce oppressive biases. Medicine tends to aim to “fix” individuals, hence further perpetuating an ableist medical model of disability. Disability is commonly viewed as a problem that exists in a person’s body and requires medical treatment; however, there is a social model of disability which, by contrast, distinguishes between impairment and disability, identifying the latter as a disadvantage that stems from a lack of fit between a body and its social environment. This will become important when discussing the data hazards that apply to this research. 

An important aim of modelling is to make clear the current state of knowledge regarding a particular system, by being precise about the elements involved and the interactions between them. Doing this can be an effective way to highlight gaps in understanding. Our understanding of experimental observations in any system can be measured by how closely the simulations we create mimic the system's real behavior. Behaviours of computer executable models are at first compared and validated with experimental values. If at this stage inconsistency is found, it means that the assumptions, that represent our knowledge on the system, are at best incomplete, or that the interpretation of the experimental data is wrong. Models that  survive this initial validation can then be used to make predictions to be tested by experiments, as well to explore configurations of the system that are not easy to investigate by _in vitro_ or _in vivo_ experiments.  Creating predictive models can give opportunities for unprecedented control over the system. Modelling can provide valuable insights into the workings and general principles of organization of biological systems.

Using computational methods for studying biological mechanisms can offer many advantages, including considerable time and cost-efficient savings. Using computer models allows us to study the specific molecules in question, for example, with the ability of simplify very complex systems by just considering the molecules immediately influencing the phenomenon that is being studied. Of course this comes with the caveat that the whole system dynamics is not being taken into account. Nevertheless, useful predictions can come out of these models to combine with wet lab research. Computer models then, are useful for creating predictions about biological phenomena. 

In biological settings, traditionally -although not always-, scientists make a hypothesis before doing the experiments, and this therefore helps guide their research for an unexplained phenomenon. So, for example, a hypothesis could be “Abolishing of CaMKII and NMDAR binding increases the amount of phosphorylated CaMKII in the PSD”. However, here we do not set a specific set of hypotheses; instead, the models created serve to combine knowledge from different published research, and make biological predictions which can then serve as hypothesis to be tested empirically by experimentalists. This project looks at biology at the level of protein-protein interactions and dynamics. It takes the view that looking at these dynamics is necessary if we want to understand emergent properties of these interactions. Therefore, modelling, simulation, and analysis of simulation outcomes are perfectly positioned for integration into the experimental cycle of cell/molecular biology. Although in-vitro and in-vivo experiments might still be needed to advance our understanding of biological processes, conducting in silico, or computer-simulated experiments can help guide the wet-lab process by narrowing the experimental search space. This in turn can mean a reduction of repeated wet-lab experiments, meaning reduced suffering of non-human animals, in accordance with the “Replacement” R of the 3Rs framework @tannenbaum2015Russell. 


--> to go with discussion of data hazards. 
Likewise, much of in-vitro research uses animal products (for example, for growing cell lines), and this uses an enormous amount of toxic chemicals as well as a huge amount of single-use plastic that cannot be recycled because of biohazard regulations. Designing better experiments, commencing from computational models, has the potential to remove waste of experiments that “won’t work” and won’t be published, which was suggested to be 85% of what a researcher produces @glasziou201685. 

In summary, some of the main reasons for using modelling are:

1.	Biological systems are complex and multiscale, models can help us to integrate experimental data, facilitating theoretical hypotheses, and addressing what if questions.

2.	Models aim to make clear the current state of knowledge regarding a particular system, by attempting to be precise about the elements involved and the interactions between them. Doing this can be an effective way to highlight gaps in understanding.

3.	Related to point one, models then serve to combine knowledge from different published research, and make biological predictions which can then serve as hypothesis to be tested empirically by experimentalists.

4.	Computer-simulated experiments can help guide the wet-lab process by narrowing the experimental search space, enabling more cost, time-effective and waste-free research, as well as more ethical research too as we reduce animal suffering through reduction of animal research.

## How do we model biochemical systems networks?
As we saw in the background section, key processes in biological and chemical systems are described by networks of chemical reactions; i.e., all the cascades of interactions between CaMKII with other molecules can be understood as a network of chemical reactions. And, as mentioned above, computational models of these reaction networks can be used to elucidate their dynamics. 

Reaction-based models are formalized as sets of reactions that describe the given system in terms of mechanistic interactions between the species of interest. This is, biochemical networks are a set of chemical species that can be converted into each other through chemical reactions. The focus of biochemical network models is usually on the levels of the chemical species and this usually requires explicit mathematical expressions for the velocity at which the reactions proceed. Biological systems can be simulated in different ways using different algorithms depending on the assumptions made about the underlying kinetics. Once the kinetics have been specified, these systems can be used directly to construct full dynamic simulations of the system behaviour on a computer.

###  What are deterministic modelling and stochastic modelling?

**Deterministic modelling:**
Deterministic approaches to chemical kinetics are often used to characterize time evolutions of chemical reactions in large systems. A popular representation for these models is to use ordinary differential equations (ODEs) to describe the change in the concentrations of chemical species. Such descriptions are appropriate when the number of particles involved in the biochemical network is large enough to be able to consider continuous concentrations and when spatial effects are negligible, i.e. well-mixed environment is assumed and space has no effect on reactions. In ODE-based models, each chemical species in the network is represented by an ODE that describes the rate of change of that species along time. Therefore, ODE models of biochemical processes are useful and accurate in the high-concentration limit, but often fail to capture stochastic cellular dynamics accurately because the deterministic continuous formulation assumes spatial homogeneity and continuous biomolecule concentrations.

These ODE models can be used to simulate the dynamics of the concentrations of the chemical species along time given their initial values. This is achieved by numerical integration of the system of ODE which can be carried out with well-established algorithms (e.g. simple forward Euler method). They are also useful to find, for example, steady states of the system, which are conditions when the concentrations of the chemical species do not change @maly2009Introduction.

**Stochastic modelling:**
Another representation that is useful in systems biology is stochastic simulations, which use probability distribution functions to estimate when single reaction events happen and therefore track the number of particles of the chemical species. As a general rule, stochastic simulations are preferred where the numbers of particles of a chemical species is small; the ODE approach is required when the number of particles is large because the stochastic approach might be computationally intractable. When the assumption of continuous concentration fails due to small-scale cellular environment with limited reactant populations, ODE representation also fails. It is here when stochastic simulations are useful. 

[//]: <> (This section about CMEs might delete, not done these references on purpose as need to revisit/reread) 

Chemical stochastic systems are usually represented by a chemical master equation (CME) that describes the time evolution of the probability distribution of discrete molecule quantities, i.e. the CME describes temporal evolution of the probability density function (PDF) for states of a chemical system. PDFs are used to describe the timing of reaction events. This evolution of probability is a continuous time Markov chain, of which any possible realizations can be generated through the Monte Carlo sampling methods. The most famous of these methods for coupled chemical reactions is the stochastic simulation algorithm (SSA) of Gillespie. The theoretical derivation of this method can be found in (Gillespie, 1976)(Gillespie, 1977) as well as a more recent review (Gillespie, 2007).

It is important to stress that one simulation run according to stochastic approaches is only one realization of a probabilistic representation, and thus provides limited amount of information on its own. When running stochastic simulations, it is very important that they are repeated for a sufficient number of times in order to reveal the entire range of behaviour presented by such a system (i.e., to estimate a distribution for each chemical species and its dynamic evolution). An example of what these model runs can look like are shown in Figure 8.

[INSERT FIGURE 8]