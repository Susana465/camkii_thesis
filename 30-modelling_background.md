---
bibliography: [references.bib]
---

# Introduction to Computational Modelling

The term "computational modelling" serves as an umbrella term, covering various types of models designed to help us understand real-world phenomena.  For simplicity, this PhD defines computational modelling broadly as "the use of computers to simulate and study complex systems through mathematics, physics, and computer science" [as described in REFERENCE fact sheet]. A computational model incorporates numerous variables representing the system under study, with simulations performed by adjusting these variables—either independently or in combination—and observing the outcomes. This PhD specifically employs computational modelling to explore biological systems, focusing on biochemical modelling of dynamic, complex reaction networks that underpin biological processes. 

The computational modelling work of this project sits within something known as complexity science. Complexity science is the study of a systems interacting parts and the emergent behaviours that appear when looking at these interactions; applications of complexity science are predator-prey models, epidemiological modelling of pandemics, protein-protein interaction networks, or models of neurons. When applied to biology, it often falls under the banner of Systems Biology. Systems biology refers to the quantitative analysis of dynamic interactions between multiple components within a biological system, with the goal of understanding the system's behaviour as a whole. Systems biology entails the development and application of systems theory principles to study complex biological systems through an iterative process of mathematical modelling, computational simulation, and biological experimentation. It serves as a tool to increase our understanding of biological systems, to develop more directed experiments, and to allow accurate predictions. 

In systems biology, it is helpful to understand and clarify what complexity means also in respect to: 

- **The model: the large number of variables that can determine behaviour.** Some hallmarks of complexity are usually the number of parameters, order of equations and evolution of networks. For example, a protein containing n peptide substrates of kinases can potentially be found in up to 2n distinct phosphorylation states. This feature of protein–protein interactions, which arises because a typical protein involved in cellular regulation contains multiple sites of post-translational modification and multiple binding sites, has been called combinatorial complexity and is a common challenge to our understanding of cellular regulation (Klamt and Stelling, 2002) (Green et al., 2018). This point relates to the explanations given below too. 

- **The natural system: the connectivity and non-linearity of relationships.** In biological systems large numbers of functionally different, and often multifunctional, sets of elements interact selectively and non-linearly to produce complex behaviours. A biological system is not always necessarily equal to the sum of its parts (REFERENCE Nurse, 1997), where functions emerge from the properties of the networks rather than from any specific element. On the contrary, in biological systems, functions rely on a combination of the network and the specific elements involved. CaMKII interaction pathways serve as a good example here too. This protein can be activated, inhibited and degraded by reactions such as phosphorylation and de-phosphorylation, while its targets are selected by the different modification patterns that exist; these are properties that reflect the complexity of the element itself.

- **The technology: the limited precision and accuracy measurements.** When modelling biological systems we encounter more layers of complexity such as, for example, looking at quantitative measures by experimental biologists: for instance, which parameters are available for modelling the system? Which ones do we infer? And in doing so, what are the model parameter sensitivity and initial values sensitivities? The popular measure of complexity for dynamical system is the computational complexity. For example, although a calculated measure in a mathematical model can characterize the amount of information necessary to predict the future state of the machine, it fails to address its meaning in the world of molecular and modular cell biology. CaMKII serves as a good case here, as mathematically it has been calculated that CaMKII’s dodecamer could have up to 10^20 states (Pharris et al., 2019). However, which ones of these states are biologically relevant is a key question that has no answer yet.

-	**The methodology: the uncertainty arising from the conceptual framework chosen.** The noise in complexity of the systems increases even further by introducing issues of robustness, noise-resonance and bi-modal behaviour. For example, once we have some outcomes in the model, how can we make sure results are robust and certain?

## Why use Computational Modelling to study biological systems?
Modelling aims to describe elements of a system of interest, their states, and their interactions with other elements. A model should be detailed and precise enough to simulate the system's behavior on a computer. Living cells are complex, with biochemical processes organized across various compartments like the extracellular space, cytosol, plasma membrane, and organelles. These processes are multiscale, involving small-scale molecular interactions that lead to emergent, larger-scale cellular behaviors. Understanding these dynamic, multiscale processes requires appropriate mathematical tools to analyze and control the system's behavior, which can help understanding life and disease.

(this is repeated from intro - need to rewrite) _An important aim of modelling is to make clear the current state of knowledge regarding a particular system, by being precise about the elements involved and the interactions between them. Doing this can be an effective way to highlight gaps in understanding. Our understanding of experimental observations in any system can be measured by how closely the simulations we create mimic the system's real behavior. Behaviours of computer executable models are at first compared and validated with experimental values. If at this stage inconsistency is found, it means that the assumptions, that represent our knowledge on the system, are at best incomplete, or that the interpretation of the experimental data is wrong. Models that  survive this initial validation can then be used to make predictions to be tested by experiments, as well to explore configurations of the system that are not easy to investigate by _in vitro_ or _in vivo_ experiments. Creating predictive models can give opportunities for unprecedented control over the system. Modelling can provide valuable insights into the workings and general principles of organization of biological systems._

Using computational methods for studying biological mechanisms can offer many advantages, including considerable time and cost-efficient savings. Using computer models allows us to study the specific molecules in question, for example, with the ability of testing minimal requirements of very complex systems by just considering the molecules immediately influencing the phenomenon that is being studied. Of course this comes with the caveat that the whole system dynamics cannot be observed. Nevertheless, useful predictions can come out of these models to combine with wet lab research and a combination of both can be useful for creating predictions about biological phenomena. 

This project looks at biology at the level of protein-protein interactions and dynamics. It takes the view that looking at these dynamics is necessary if we want to understand emergent properties of these interactions. Modelling, simulation, and analysis of simulation outcomes are well positioned for integration into the experimental cycle of cell/molecular biology. Although _in vitro_ and _in vivo_ experiments might still be needed to advance our understanding of biological processes, conducting _in silico_, or computer-simulated experiments can help guide the wet-lab process by narrowing the experimental search space. This in turn can mean a reduction of repeated wet-lab experiments, meaning reduced suffering of non-human animals, in accordance with the “Replacement” R of the 3Rs framework @tannenbaum2015Russell. 

_(From Pharris paper) To characterize the spatiotemporal regulation of CaMKII, experimental studies are increasingly complemented by computational models [15, 17, 25–28]. Computational models of Ca2+-dependent signaling implicate competition, binding kinetics, feedback loops, and spatial effects in regulating enzyme activation [7, 12, 24, 29, 30]. However, fully characterizing these and other mechanisms of CaMKII regulation is impeded by the challenge of accurately portraying the CaMKII holoenzyme. As described by previous work, combinatorial explosion can occur when modeling CaMKII (and similar biomolecules) activation because the protein exhibits a large number of functionally significant and not necessarily inter-dependent states [24, 26, 31–33]. The large number of possible states of CaMKII can neither be explicitly specified nor efficiently evaluated with conventional mass action-based methods. Indeed, for just one CaMKII hexamer ring, we estimate a state space of ~32 billion states, and for the full dodecamer approximately 1020 possible states (See Text A in S1 Appendix). The numbers of possible CaMKII states far exceeds the number of CaMKII molecules in a dendritic spine, suggesting that some states rarely occur and thus likely contribute little to protein function. Previous models leverage this observation to reduce the model state space and provide valuable insight to CaMKII binding and autophosphorylation dynamics [24, 33–36]. However, for CaMKII it remains unclear which states functionally participate in synaptic plasticity. Reduced models can inadvertently obscure key mechanisms regulating CaMKII activation and autophosphorylation. To elucidate complex regulatory mechanisms, it may be necessary for models to provide for all possible states ab initio._

## How do we model biochemical systems networks?

Biological and chemical systems can be described by networks of chemical reactions; in other words, the cascades of interactions between CaMKII and NMDARs with other molecules can be understood as a network of chemical reactions. Computational models of these reaction networks can be used to elucidate their dynamics. Reaction-based models are formalized as sets of reactions that describe the given system in terms of mechanistic interactions between the species of interest. This is, biochemical networks are a set of chemical species that can be converted into each other through chemical reactions. The focus of biochemical network models is usually on the levels of the chemical species and this usually requires explicit mathematical expressions for the velocity at which the reactions proceed (reaction rates). Once the kinetics have been specified, these systems can be used directly to construct full dynamic simulations of the system behaviour on a computer. Biochemical network models allow us to gather insight by simulating chemical interactions over time; we can observe changes in species levels, visualise stable states within the system, and look for potential direct or indirect causal relationships between the species being studied. Additionally, we, the modellers, can modify any of these parameters to test how such changes impact the model’s results.

Biological systems can be simulated in different ways using different algorithms depending on the assumptions made about the underlying kinetics, as we will see below; and different formalisms are usually applied to describe the dynamics of these biochemical systems. The kinetics of chemical reactions vary based on the timing of molecular interactions, with reaction processes unfolding over a timescale determined by the microscopic mechanics involved —such as molecular collisions driven by Brownian motion, electron transfer, and similar interactions. Molecular collisions occur randomly, influenced by factors like thermal motion and diffusion. This randomness means that the number of molecules of a particular species fluctuates as a random variable. However, when we observe large-scale, or macroscopic, quantities —such as the concentration of a substance over time— the outcomes tend to be consistent and predictable. This predictable trend enables us to develop rate laws, mathematical expressions that describe how the concentration of molecules changes over time. Rate laws are foundational to deterministic modelling, as they assume that, given a specific starting point (initial conditions), the progression of a chemical process is fixed or "predestined." Deterministic models thus allow scientists to predict the time evolution of chemical concentrations with high accuracy, even if the underlying molecular interactions remain random on a microscopic scale. Deterministic models work well where molecular species exists in vast quantities. However, as systems decrease in scale -such as in the confined environment of a cell's cytosol- random fluctuations in molecular populations become significant, making experimental results less reproducible and measurements more variable. Unlike deterministic models, which assume smooth, predictable changes, stochastic models accommodate the random fluctuations in molecule numbers that can significantly impact reaction outcomes in confined environments. Lets examine the reasons why each of these approaches may be employed for distinct purposes:

### Deterministic and stochastic modelling {.unnumbered}

**Deterministic modelling:**
Deterministic approaches to chemical kinetics are often used to characterize time evolutions of chemical reactions in large systems. A popular representation for these models is to use ordinary differential equations (ODEs) to describe the change in the concentrations of chemical species. Running the same set of parameters using deterministic simulations will produce the same results each time by solving these ODEs. Such descriptions are appropriate when the number of particles involved in the biochemical network is large enough to be able to consider continuous concentrations and when spatial effects are negligible, i.e. well-mixed environment is assumed and space has no effect on reactions. In ODE-based models, each chemical species in the network is represented by an ODE that describes the rate of change of that species along time. Therefore, ODE models of biochemical processes are useful and accurate in the high-concentration limit, but often fail to capture stochastic cellular dynamics accurately because the deterministic continuous formulation assumes spatial homogeneity and continuous molecular concentrations.

These ODE models can be used to simulate the dynamics of the concentrations of the chemical species along time given their initial values. This is achieved by numerical integration of the system of ODE which can be carried out with well-established algorithms (e.g. simple forward Euler method). They are also useful to find, for example, steady states of the system, which are conditions when the concentrations of the chemical species do not change @maly2009Introduction.

**Stochastic modelling:**
Another representation that is useful in systems biology is stochastic simulations, which use probability distribution functions to estimate when single reaction events happen and therefore track the number of particles of the chemical species. As a general rule, stochastic simulations are preferred where the numbers of particles of a chemical species is small; the ODE approach is required when the number of particles is large because the stochastic approach might be computationally intractable. When the assumption of continuous concentration fails due to small-scale cellular environment with limited reactant populations, ODE representation also fails. It is here when stochastic simulations are useful. 

It is important to stress that one simulation run according to stochastic approaches is only one realization of a probabilistic representation, and thus provides limited amount of information on its own. When running stochastic simulations, it is very important that they are repeated for a sufficient number of times in order to reveal the entire range of behaviour presented by such a system (i.e., to estimate a distribution for each chemical species and its dynamic evolution). An example of what these model runs can look like are shown in Figure 8.

### Rule based modelling {.unnumbered}

An intricate network of protein–protein interactions is a prominent feature of any signal-transduction system @gomperts2009Signal, @hunter2000Signaling. Yet despite the high relevance of modelling site-specific details on protein-protein interactions of signalling systems, there has been a lack of standards for explicitly representing the composition and connectivity of molecular complexes @ezkurdia2009Progress. Models that incorporate complex protein-protein interaction details are generally difficult or impossible to specify and analyse, largely because of the combinatorial number of protein modifications and protein complexes that can be generated through protein–protein interactions. This is known as combinatorial complexity, which is a common challenge to our understanding of cellular regulation. We will see below how this can be resolved with rule based modelling, and more specifically with BioNetGen tools.

There are three main things that are important here: space, time and structure of molecules (?). 

the way compratments work in this kind of modelling - will need a methods section that is more specific than this one i think.


### Software used in this PhD {.unnumbered} - MIGHT need to be a chapter in itself


In summary, some of the main reasons for using modelling are:

1.	Biological systems are complex and multiscale, models can help us to integrate experimental data, facilitating theoretical hypotheses, and addressing what if questions.

2.	Models aim to make clear the current state of knowledge regarding a particular system, by attempting to be precise about the elements involved and the interactions between them. Doing this can be an effective way to highlight gaps in understanding.

3.	Related to point one, models then serve to combine knowledge from different published research, and make biological predictions which can then serve as hypothesis to be tested empirically by experimentalists.

4.	Computer-simulated experiments can help guide the wet-lab process by narrowing the experimental search space, enabling more cost, time-effective and waste-free research, as well as more ethical research too as we reduce animal suffering through reduction of animal research.
