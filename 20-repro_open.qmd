---
bibliography: [references.bib]
---
# Open, Reproducible and Ethics-focused PhD {#sec-open-repro-chapter}
In recent decades, scientific research has accelerated significantly, with a "publish or perish" culture becoming deeply ingrained in the academic community’s consciousness [@angell1986Publish; @rawat2014Publish]. Frequent publication is widely regarded as a powerful tool for scholars to showcase their expertise and gain recognition among their peers. Likewise, there is a generalised view that this keeps academics constantly engaged with relevant knowledge works in their fields of expertise. In theory, it also serves to benefit the wider public by making scholarly insights available. Successfully published research not only enhances an individual’s reputation but also raises the profile of their institution, often attracting greater funding opportunities. Moreover, academic institutions and universities commonly use the number of publications credited to a researcher as a key indicator of their competence and career progression [@york2015Defining; @aksnes2019Citations;@elbanna2023Publish]. In this view, a strong publication record can be beneficial, opening up opportunities for researchers and their institutions. 

However, this mindset exerts significant pressure on researchers to continuously publish in order to secure funding and advance their careers [@miller2011Publish; @hangel2017Why]. Alongside this, there are persistent biases in academic publishing, where work affiliated with prestigious institutions is often (wrongly) perceived as higher quality than that from less well-known or under-resourced institutions, regardless of the actual merit of the research [@morley2007Employers; @paradeise2013Academic; @campbell2019Prestige]. On top of this, the Eurocentric bias where academic knowledge produced in wealthier countries is often regarded as superior, marginalises and even dismisses the realities and contributions of the Global South [@jazeel2010Limits; @jyothis2016NeuroOppression; @castrotorres2022North; @amutuhaire2022Reality]. Similarly, the expectation for a high volume of publications impacts disproportionately low-paid academics in resource-constrained countries, who can face a variety of structural barriers, including limited grant funding [@castrotorres2022North] and inaccessible academic journal costs [@chan2014Increasing; @collyer2018Global] among many other reasons [@hopkins2013Disparities; @willis2021Gender]. 

In the culture of continuously publishing and publishing fast, those who are already at a disadvantage due to systemic bias and oppression are the ones who continue to lose the most. For example, research has shown that minoritised communities —whether this be due to race, gender, sexual orientation, or other factors— are underrepresented in prestigious journals, partly due to implicit biases and the dominance of Western academic frameworks [@hopkins2013Disparities; @willis2021Gender; @boda2022What]. Consequently, the "publish or perish" culture overwhelmingly benefits scholars in wealthier countries while deepening global inequalities in knowledge production.

Moreover, a pressure to publish in high impact journals is closely tied to what is known as publication bias. In published academic research, publication bias occurs when the outcome of an experiment or research study biases the decision to publish or otherwise distribute it. This typically results in a preference for publishing significant or positive results, disrupting the balance of findings and favouring those that are deemed novel or noteworthy. Whilst negative or inconclusive results are rarely published or considered less worthy of publication [@thornton2000Publication; @page2021Investigating].

Examining this bias towards positive results reveals substantial evidence of a troubling trend: an overrepresentation of false-positive findings in the scientific literature [@sinha2006Reporting; @esarey2016Measuring; @hartgerink2017Too; @stahl2018Fact; @schneck2023Are]. This bias towards positive results has several concerning and wasteful consequences, including, but not restricted to: a significant number of valid negative results remaining unpublished, which excludes critical findings from the scientific record. This means that other research teams, unaware of these unpublished negative results, may continue to unknowingly test the same hypotheses (which may actually be false) until, by chance or artifact, a positive result is obtained. These chance positive results are then published, as they align with the preference for significant findings, even when substantial and more definitive contradictory evidence may exist in contrast to it [@carlson2012Understanding]. What is more, this underreporting of negative results introduces bias into meta-analysis, which consequently misinforms researchers, doctors, policymakers and the public in general [@afonso2024Perils]. Additionally, more resources are wasted on already disputed research that remains unpublished and therefore unavailable to the scientific community [@kicinski2013Publication; @kicinski2014How; @aert2019Publication; @page2019Assessing]. 

This cycle sustains an error-prone body of scientific literature, undermining the reliability of published research. Hand in hand with this publication bias, comes something now known as a _Reproducibility Crisis_, where it is becoming increasingly apparent that making "fast-science", while not the sole cause, is associated with a lack of being able to reproduce research [@stengers2016Another; @leite2021Juggling]. The reproducibility crisis has been in the rise for the past few decades as we uncover the fact that much of the research that is published fails to be reproduced by others. To give some examples, a survey of 1576 scientists published in Nature [@baker2016500ScientistsLift] reported that over 70% of the participants failed to reproduce others’ experiments and over 50% failed to reproduce their own results. Similarly, @tiwari2021ReproducibilitySystemsBiology assessed the reproducibility of 455 mathematical models in systems biology and found that about 50% of published models were not reproducible either due to incorrect or missing information in the manuscript.

Much of the criticism surrounding the reproducibility crisis centres on statistical methods and research practices [@ioannidis2005Whya; @friese2020PHacking; @stefan2023Big]. Problematic scientific behaviours, such as HARKing (Hypothesising After the Results are Known), p-hacking (manipulating data analysis to achieve statistical significance), and selectively reporting only positive outcomes, as discussed above, have been identified as major contributors to irreproducibility [@wagenmakers2007Practical; @nuzzo2014Statistical; @lew2020Reckless]. Indeed, this reliance on p-values, provided they meet the conventional threshold of statistical significance (typically p < 0.05), can sometimes lead to studies being treated as definitive evidence, even when their findings are not robust or reproducible [@lew2020Reckless]. 

The list of factors contributing to irreproducibility is extensive, and is not restricted to statistical methods. A lack of access to raw data or, in some cases, outright data fabrication also plays a role [@flier2017Irreproducibility; @bausell2021Problem]. Ambiguities in experimental procedures and data analysis steps further undermine research reliability. On a broader, more systemic level, there's an important discussion regarding how current academic systems often prioritize novelty and statistically significant findings, as research funding is more likely to be secured and promoted when the outcomes are more profitable [@bhandari2004Association; @lynch2007Commercially; @ebadi2016How]. Although these behaviours fall under scientific misconduct [@science2023Reproducibility] and are not considered acceptable scientific practice, they continue to occur as the mindset of publish or perish continues to embed itself in how scientists work.

Interestingly, a large-scale survey of nearly 6,000 academic psychologists (with 2,155 responses) assessed self-reported engagement in some of these questionable research practices (for example ambiguities in experimental procedures, p-harking, etc) known to introduce bias into research findings [@john2012Measuring]. Notably, respondents often justified their own use of these practices while simultaneously viewing them as unacceptable when used by others (p. 530). This is just one example, and it is worth noting that scientific standards and methodologies continuously evolve, meaning that questionable research practices are not static. Methods once tolerated, or even considered standard, may now be recognised as problematic, reflecting our ability to reassess and refine ethical and methodological frameworks. Encouragingly, awareness of the reproducibility crisis is growing, and there is a gradual shift towards promoting open and reproducible research practices [@umbach2024Open, @nature2018Challenges].

Together, the issues outlined above, from the reproducibility crisis and publication bias to the prevalence of questionable research practices, reveal not isolated failings, but a systemic entanglement that compromises the integrity, efficiency, and ethical foundation of scientific research. In this context, the need for a fundamental shift becomes clear: one that centres reproducibility and accessibility not as optional enhancements, but as essential to producing ethical and meaningful research. 

While I cannot solve these systemic issues alone, this PhD is an opportunity to actively engage with them. I am approaching my research with a conscious commitment to reproducibility, transparency, and accessibility, while also taking seriously the ethical responsibilities that come with producing and sharing knowledge. By embedding these values into the way I design, conduct, and communicate my work, I hope to contribute to a broader shift towards more inclusive, accountable, and responsible research practices.

## Reproducibility Definition Used in this PhD
There is a long history of the terms reproducibility and replicability being used interchangeably, or their meanings being swapped depending on the field of study [@claerbout1992ElectronicDocumentsGive; @ivie2018ReproducibilityScientificComputing; @plesser2018ReproducibilityVsReplicability]. For example, a review on the usage of reproducible/replicable meanings [@barba2018TerminologiesReproducibleResearch] showed that most papers and disciplines use the terminology as defined by Claerbout and Karrenbach, whereas microbiology, immunology and computer science tend to follow the Associtation for Computing Machinery use of reproducibility and replication given by [@ivie2018ReproducibilityScientificComputing]. In political science and economics literature, both terms are used interchangeably.

In this PhD, we use the definition used by [@turingwaycommunity2019TuringWayHandbook], where reproducible research is understood as "_work that can be independently recreated from the same data and the same code that the original team used_". Reproducible, replicable, robust and generalisable have different meanings as described in the table below.

![How The Turing Way defines reproducible research. Adapted from @turingwaycommunity2019TuringWayHandbook.](22-reproducibility-figures/repro_definintions.PNG){#fig-repro-definitions width=65%}

- **Reproducible research**: is obtained when same analysis is performed on the same data, to produce the same results.

- **Replicable research**: refers to conducting the same analysis on different datasets, resulting in qualitatively similar outcomes. 

- **Robust research**: entails subjecting the same dataset to different analysis workflows to address the same research question, such as employing distinct pipelines in R and Python. Robustness demonstrates that findings can remain consistent regardless of different methods used for analysis, indicating validity and resilience to various factors like changes in conditions or methods (such as different programming languages).

- **Generalisable research**: refers to findings or conclusions that can be applied beyond the specific context in which they were derived. It indicates that the results are not limited to a particular dataset, methodology, or experimental setup, but instead can be extended to broader populations, situations, or conditions. By combining replicable and robust research, we can obtain more generalisable results.

## Open Science
Open Science is an approach to the scientific process that promotes cooperative work and new ways of diffusing knowledge accessible to everyone, without barriers such as paywalls or restrictions on the use of research outputs. By making research more accessible and transparent, open science seeks to enable more efficient scientific progress, enhance reproducibility, and increase the societal impact of research findings. A definition provided by @vicente-saez2018Open gives an overall good idea of what open science is referring to: "_Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks, [it] helps the scientific community, the business world, political actors, and citizens [...] and stimulates an open debate about the social, economic, and human added value of this phenomenon._" Additionally, the United Nations Educational, Scientific and Cultural Organization (UNESCO) promote the following message in their Recommendation on Open Science: "_By promoting science that is more accessible, inclusive and transparent, open science furthers the right of everyone to share in scientific advancement and its benefits as stated in Article 27.1 of the Universal Declaration of Human Rights_".

Open Science includes related concepts such as Open Data [@Whata], Open Source [@Open] and Open Access [@suber2012Open] among others [@Opena; @parsons2022Communitysourced; @turingwaycommunity2019TuringWayHandbook; @Opena]. **Open Data** refers to the openness and accessibility of data. It involves thinking (and acting) about data sharing, privacy, and protection, as well as considerations like consent and the nuances of handling sensitive data. In the context of computational projects like this PhD, **Open Source** applies to both software (e.g., programs and applications used) and hardware (e.g., types of machines involved) that are released under **Open Licensing**, making them publicly accessible for anyone to view, modify, use, and distribute. [@Guide; @kreutzer2014Open]. **Open Access** refers to how freely available research content is. There are different ways to achieve this; some times, open access might involve paying an Article Processing Charge to a journal, which then publishes the final version of the article under an open license [@borrego2023Article], making it permanently free to access online (this is known as Gold Open Access). Sometimes, it may involve self-archiving a version of the research, often alongside preprints, allowing public access without direct journal fees (this is known as Green Open Access) [@bjork2014Anatomy].

In order to achieve Open Science, the research process should:

1.	**Be publicly available**: It's hard to benefit from knowledge hidden behind barriers like passwords and paywalls.

2.	**Be reusable**: Research outputs should be licensed adequately, informing potential users of any restrictions on reuse.

3.	**Be transparent**: With appropriate metadata to provide clear statements of how research output was produced and what it contains.

Additionally, Open Science and its various elements are directly related with the broader concept of **Open Scholarship** [@scanlon2014Scholarship; @tennant2019Foundations]. Open Scholarship promotes transparency and accessibility in teaching, learning, research, and academia [@emeryLibGuides]. More importantly, Open Scholarship emphasizes equity, diversity, and inclusion, ensuring that knowledge is openly available to everyone, regardless of ethnicity, gender, sexual orientation, or other protected characteristics.

It is worth noting that Open Science does not mean “sharing absolutely everything”. Many fields of science involve working with sensitive personal data, with medical research being the most obvious example, where data is not to be widely shared. Likewise, privacy and data protection, as well as consent, and national and commercially sensitive data can be some of the most common examples of when data cannot always be open [@regulation2016RegulationEU2016]. If access to data needs to be restricted due to security reasons, however, the justification for this should be made clear. Free access to and subsequent use of data is of significant value to society and the economy. The concept of Open Science views that data should, therefore, be open by default and only as closed as necessary.

## FAIR Principles for Open and Reproducible Science
Weaved in with the topics already discussed, are the FAIR principles for scientific data management. These principles were created as a guideline to develop and collectively support a clear and measurable set of principles that allow for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability of digital assets, to ultimately support more reproducible research [@wilkinson2016FAIRa]. FAIR principles, therefore, serve as a valuable framework for conducting research with integrity. 

The advantages of applying FAIR principles become apparent when we understand each principle [@cole2017Using; @kremen2018Improving; @kim2024Findability]. Specifically linked to computational projects such as this PhD, FAIR practices can lead to more efficient code, minimising time spent on things like model retraining/rewriting, and reducing redundant data generation and storage [@hasselbring2019FAIR; @borrego2023Article]. Likewise, the interoperability aspect of FAIR principles enables researchers to integrate datasets from various sources, fostering new insights and innovative problem-solving approaches. For example, application of FAIR principles can result in lowering carbon footprint of computer simulations as less time and resources are wasted trying to run models that lacks useful information on how to run it [@lannelongue2023GREENER].

It is not uncommon for scientific code and workflows to originate within small, specialised research groups, often resulting in ad-hoc code that functions as a 'black box' to external researchers and developers. As this process unfolds and mixes with the fast-paced mindset mentioned above, if a workflow and materials are not clear from the beginning, future researchers may prioritise immediate functionality over long-term code quality, often due to constraints such as tight deadlines, limited resources, or evolving project requirements, causing something known as technical debt where the long term costs of workflow opacity costs human effort, money and time. One of the goals of this thesis project is to provide a workflow that enables reusability at initial creation and at the time of reuse, so that technical debt is decreased and the barriers for driving community interactions and innovations are lowered as a result.

## Important steps taken to apply FAIR principles, Open Science by Design and Reproducibility
While extensive literature and tutorials exist on best practices for reproducibility, the following provides a personal account of the tools and methods I have employed, along with their practical benefits in creating a computational model that others can reproduce. This section not only demonstrates the practical implementation of these practices but also showcases this PhD as an example of working towards a reproducible, open, and ethics-focused PhD in Computational Neuroscience. I reflect on how I have engaged with these principles, adopted the most relevant practices for this project, and integrated them into my research. 

I begin by outlining the steps taken to align with each of the FAIR principles, before exploring additional strategies that, while extending beyond the FAIR principles, still support FAIR research overarching goals.

### Findable:
_Data is well-described with rich metadata, making it easier to locate by both humans and machines_ [@jacobsen2020FAIR].

- The model and thesis are available in GitHub repositories, where people can access and find data.
- The dataset will be available with a DOI upon finalization [insert DOI once ready].
- Presentations, workshops, and papers are available with persistent identifiers where applicable [REFERENCE REPOS AND DOIs].
- Metadata used in the model follows human-readable naming conventions while still being domain-specific within neuroscience. Although some terminology is niche, notations and explanations are provided [see code link].

### Accessible
_Data is stored in a way that ensures users can retrieve it with clear licensing and open protocols_ [@jacobsen2020FAIR].

- The models created during this PhD project are open and accessible through GitHub, with documentation explaining their use and specifications.
- Related to the point above, README files provide clear guidance on protocols and explain how each repository works and what they contain, including licensing where appropriate. 
- As well as models, workshops and presentations associated with my research are also available in public repositories such as Zenodo (and GitHub) [REFERENCES]. These platforms are invaluable in making the tools and materials I have developed publicly accessible, while also offering mechanisms for proper citation through DOIs and appropriate licensing, where applicable. 
- Data is stored in [chosen database REFERENCE], ensuring long-term accessibility.
- Research resulting from this PhD has been made publicly accessible through preprints [@garcia2024Data] (Green Open Access, allows access without the need for direct journal fees [@bjork2014Anatomy]), and Open Access publication when publishing in journals [@[@garcia2025Data]], making it permanently free to access online (this is known as Gold Open Access [@borrego2023Article]).

### Interoperable
_Data is formatted using standardised vocabularies and structures, enabling seamless integration across different applications or workflows_ [@jacobsen2020FAIR].

- Interoperability is particularly relevant to this project, as the model needs to interface with multiple software tools (e.g., BioNetGen, MCell, Python) [see CHAPTER-REFERENCE]. By structuring the data and code to be compatible across these platforms, I ensure that the model can be integrated with future datasets and incorporated into diverse workflows for analysis, storage, and processing.
- The scripts written in Python utilize libraries that facilitate cross-software integration, making the model adaptable [see CHAPTER-REFERENCE].
- The nomenclature in this project follows similar vocabulary/naming of molecules, albeit with slight variances in naming as the model, molecules functions and behaviours are not the same as other models. However, using BioNetGen Language and Python allows for a formal, accessible, shared and broadly applicable language.

### Reusable
_Data is well-documented, with detailed provenance and clear usage rights, supporting reproducibility and future research_ [@jacobsen2020FAIR].

- I have provided a detailed description of the model and its components, ensuring that (meta)data are richly annotated with relevant descriptors and attributes. Specifically, I outline the specifications for usage of data, including kinetic rates, molecule reaction rule names, and their sources or calculation methods.
- Where appropriate (for example when using GitHub repositories), I have explicitly licensed the data and stated usage rights, making it clear how others can build upon this work.
- The model includes comprehensive provenance details [see table of reactions with references], documenting data origins, derivation methods, and any transformations applied. 

### Good Computational Research Practices
In addition to the steps outlined above in alignment with the FAIR principles, this section elaborates on the broader computational practices adopted throughout this PhD to further support the goals of reproducibility and open science. These practices include maintaining comprehensive documentation of code and methods, managing computational environments to ensure consistency, carefully tracking and addressing errors, and using version control to log changes and facilitate collaboration. These practices form the foundation of reproducible research and have been central to building a workflow that is transparent and accessible to others.

#### Comprehensive Documentation {.unnumbered}
Ensuring that my code is clear, well-documented, and reusable has proven essential for both my own future reference and for others who may build upon this work. Indeed, code that is human readable with useful comments (see The Turing Way for guidelines on good code comment practices [@turingwaycommunity2019TuringWayHandbook]) not only enhances reproducibility but also facilitates collaboration by making it easier to understand, modify, and extend. One of the key strategies I have implemented to achieve clarity and reusability is incorporating detailed and clear comments throughout the code to explain its purpose, logic, and key functions has been essential to help anyone (including my future self) quickly grasp what each section does without having to decipher it from scratch. Comments also help to clarify why certain decisions were made, which is crucial for long-term sustainability of the project.

Beyond inline comments, comprehensive documentation has also been key. I have documented the broader aspects of my computational work, including the software, hardware specifications, and version numbers used [LINK SECTIONS -REFERENCE]. This ensures that the research can be replicated under similar conditions, minimising inconsistencies caused by version mismatches or dependency issues. Likewise, each repository includes a README file [@githubREADMEs] that provides an overview of the project, instructions for installation, dependencies, and usage guidelines. This serves as an entry point for users, making it easier to navigate the codebase and understand how to run and interpret the scripts.

#### Computational Environments {.unnumbered}
I run the model on my local machine, as well as on the university’s high-performance computing (HPC) cluster. Additionally, if my supervisors or collaborators wish to run the model on their own machines, their computer environments will be different again. So the model is bound to be run in different environments depending on who's running it and where. Every computer operates within a unique computational environment, encompassing its operating system, installed software, specific versions of packages, libraries, and system configurations. 

Since the model depends on these components, ensuring compatibility across different setups is essential for seamless execution. Even minor variations in library versions can cause unexpected errors or inconsistencies in results, or worse, prevent the code from running entirely [@spinellis2003Code]. To address these challenges, as well as the comprehensive code and documentation, I have created computational environments using package management systems [see defintion @turingwaycommunity2019TuringWayHandbook] that collaborators can use in their own machines when running the model. Computational environments act as, as per their name, "containers/environments" where all required software, libraries, and packages are pre-defined, allowing the model to run under controlled conditions [@gruning2018Practical; @maji2020Demystifying]. In this project, I use Conda (https://conda.io) as my preferred package management system for creating environments, as I find it the most intuitive and flexible. However, other environment managers can be used depending on user preference. These environments allow specific versions of software and libraries to be installed without affecting the global system setup. This ensures that the model runs identically on any machine with the same environment, providing a robust solution for running the same model on different machines. 

![As an example of how environments work, the model is initially created on Machine 1, which runs the Windows operating system, has MCell version 4 installed, and uses Python version 4.1. To ensure that the model can be successfully executed on a different system, a dedicated computational environment is set up, capturing all the necessary dependencies. The environment setup and the model files are uploaded to a GitHub repository. This repository can contain the environment.yml file listing the required dependencies, including Python 3.9, NumPy, and pandas, as well as the model files (ABC_model.py and ABC_model.bngl), and a README.md file with instructions (such as downloading MCell version 4). On Machine 2, which runs Linux, has MCell version 3.3, and Python version 3.8, the user can, according to README instructions, update MCell to version 4 and use the specified environment by activating the environment.yml file. The model can be run within this contained environment.](22-reproducibility-figures\environment.PNG){#fig-environment fig-pos='H'}

#### Prepare for Errors {.unnumbered}
Regularly testing code is, unsurprisingly, helpful for ensuring reliable and reproducible research. Throughout my PhD, I have incorporated various testing strategies to improve the reliability of my code. One key practice has been anticipating and preparing for errors, which has helped me catch potential issues before they become major problems. This is nothing ground-breaking but rather following good-practice coding, such as readable, commented code, as explained above, having print statements to know what the code is doing where and when, error handling and creating validation steps throughout. For me, creating validation steps involved carefully adding reactions one by one, running the code, and confirming that the molecules were reacting as expected —or at least within a reasonable range— since the model is stochastic (see @sec-how-do-we-model) and may exhibit emergent behaviors that were not predicted. I have also learned to validate my code step by step through debugging [@mccauley2008Debugging], the process of identifying, analyzing, and fixing errors or bugs in a program, to break down how the code is working and pinpoint where things go wrong. Additionally, I have used mock or dummy test models, such as the [ABC_model -reference], to simplify testing when the main model is too complex to easily trace errors. 

#### Version Control {.unnumbered}
Version control is a pillar of most, if not all, guides for reproducible research [@sandve2013Ten; @stodden2014Implementing; @sullivan2019Open; @turingwaycommunity2019TuringWayHandbook; @peikert2021Reproducible; @hamra2025Advancing]. Version control is a method for tracking and managing changes to files or projects over time. It allows you and your collaborators to monitor modifications, review past edits, and restore previous versions if needed. Using version control helps maintain a relatively organized and traceable record of updates across different stages of development. This is extremely helpful for times when I needed to check back on when and where I did a specific change to my work, or when working collaboratively to know who wrote what and when. In terms of reproducibility, this means that version control helps with having clear provenance of information [@hamra2025Advancing]. As a result of using version control, I have made my life easier (and potentially others in the future too) as I could track what version of the code and data produced specific outputs, for example. As datasets grow larger and more complex, by having a solid version control workflow you are doing your future self (and collaborators) a huge favor. 

There are various ways to implement version control [@atkins2002Using; @pilato2008Version; @milentijevic2008Version; @hethey2013GitLab], but I primarily use Git and GitHub [@loeliger2012Version] —partly because they are widely adopted and partly because they were the tools introduced in the version control courses I attended. With Git, each version update (or "commit") can include a message describing the changes made, such as "changed value x=1 to x=2." This has made it much easier to track modifications over time and follow the evolution of this project. Version control is therefore, especially useful when sharing analyses, as it ensures transparency, reproducibility, and auditability -key aspects of good scientific practice.

##### Gradual Development and Validation {.unnumbered}
The way I built the model for this project involved gradually developing the code; for example, starting with CaMKII as a monomer, then a hexamer, and finally a dodecamer. This step-by-step approach ensured proof of concept and allowed me to develop the model incrementally, building on solid, validated foundations before moving to the next stage. Git’s branching feature made this process much smoother, as it let me work on different aspects of the model in separate lines of development without affecting the main project. This meant I could experiment, test, and refine changes in parallel workspaces, keeping the main codebase stable until updates were ready to be merged. Furher details on model validation can be found in @sec-validating-model.

##### Data Management {.unnumbered}
With this gradual development, the datasets evolved, variable names changed, and file hierarchies were adjusted throughout. As a result, managing large data outputs eventually became a challenge (see Challenges in Version Controlling Data described by @turingwaycommunity2019TuringWayHandbook). For example, GitHub imposes a limit of 100MB per file, preventing the storage of large files directly within the repository. Whilst Git can be used for data versioning by installing tools like git-annex, Git LFS, or git submodules, this means integration of more complex tools, which can introduce accessibility issues for newcomers. For example, with a data version control add-on tool like DataLad (which builds upon git and git-annex (https://git-annex.branchable.com/)), users may be able to see the files in the repository, but they won't be able to access or download it directly, instead they would have to clone the entire repository, which may be far from ideal. This added complexity can be a barrier for those who are not familiar with the tools, potentially hindering the ease of use for collaborative projects.

To address data versioning, I implemented various strategies. Firstly, timestamping output files: I saved output files with timestamps rather than overwriting them. This practice ensures that each file's creation time is preserved, although it does not allow for version control of those files. While this means any modifications to the files won't be tracked automatically, the documentation clearly specifies that any updates to the files should be reflected in the file names, maintaining clarity and transparency. Secondly, I developed a simplified version of the model, called the ABC_model, which mirrors the workflow of the main CaMKII_dodecamer_model. This model tracks fewer molecules (e.g., a + b -> c) and produces less complex, smaller output files. Although this doesn’t address the version control of the primary model's data, it is an effective tool for monitoring how outputs are generated and understanding their structure during testing phases. Additionally, I uploaded final outputs to a database: once the final, validated outputs were produced, I uploaded them to a database for proper storage and sharing. This step ensures that the final, reproducible data is securely stored and accessible. Likewise, packing data output files into smaller sized files (for example, zipping them), allows for data to be tracked, albeit, sometimes this is not a viable solution because data may still be too big.

##### Quarto for Version-Controlled Documents {.unnumbered}
In addition to maintaining version control of the model(s), I also used Quarto [REFERENCE], an open-source publishing system designed for scientific and technical writing. Quarto enables the dynamic generation of various file formats, including Markdown, LaTeX, HTML, PDF, and Word, all of which can be version controlled. This functionality has been invaluable in creating a structured, traceable workflow within a single, integrated system. You can see various examples of quarto documents I have created, including this thesis here [INSERT REFERENCES]. By leveraging Quarto’s flexibility, I have been able to write my thesis in Markdown, a highly intuitive and lightweight markup language, while keeping individual .qmd (Quarto Markdown) files under version control. Quarto then renders these files into my preferred output format, including PDF, Word, LaTeX, among other formats. This approach has been significantly more efficient than the previous ad-hoc method of manually tracking document versions (e.g., final, final_forreal, final_forreal2), which lacked transparency and made it difficult to trace specific changes. With Quarto, I can use version control of the files and precisely track modifications, review previous iterations, and revert to earlier versions when necessary, greatly improving both the organisation and reliability of my work.

#### Automation for Efficacy and Sustainability {.unnumbered}
Beyond ensuring clarity, documentation, and reproducibility, automation plays a key role in making code more sustainable. By automating repetitive tasks, I reduce the likelihood of human error, save time, and ensure consistency across different runs of the model. This is particularly important when working with complex simulations, large datasets, or high-performance computing (HPC) environments, where manually setting up and executing processes can be inefficient and error-prone. 

One example of automation in this project is the use of scripts for setting up environments and running simulations. Instead of manually installing dependencies and configuring settings each time the model is used, I have created scripts that automatically set up the required computational environment and execute the necessary commands [REFERENCE README IN GITHUB]. This eliminates the risk of missing dependencies, ensures the correct software versions are used, and reduces the setup burden for new users. Additionally, I have automated data processing workflows to streamline the handling of simulation outputs. Large-scale simulations generate significant amounts of data, and manually processing these results would be time-consuming and inconsistent. By writing reusable scripts for tasks such as data cleaning, aggregation, and visualization, I ensure that results are processed systematically and efficiently. This not only saves time but also makes it easier to compare outputs across different runs, improving the reproducibility of results.

## Reflections and Future Improvements

Data version control has been hard, and in hindsight it would have been helpful to plan even more in advance. time constraints were real; when there are so many things one can do, there is not time for everything; although I have put lots of effort into reproducibility, there is always room for imprevement, for example automation of tasks could be fenessed even more by creating python scripts that can be even more ptimized (how though?). preparing for errors could have also been improved through more official things like unit testing and more scientifcally known testing methods...

_In summary, by adhering to the FAIR principles, I have maintained a structured approach to version control, using tools like Git and GitHub to track changes, ensure transparency, and facilitate collaboration. The gradual development of my model, alongside careful versioning of code and data, has allowed me to manage evolving datasets and refine my work incrementally. Additionally, I have prioritised writing clear, reusable code with comprehensive documentation and established computational environments to ensure the model runs consistently across different systems. Through consistent code testing, including unit testing, error handling, and validation steps, I have been able to identify and address potential issues early, ensuring the stability and accuracy of the model. The use of Quarto for writing my thesis has further enhanced this process by enabling version control and structured writing, ensuring that each stage of my research is traceable and reproducible. Together, these practices have laid a strong foundation for conducting rigorous, transparent research that can be more easily understood, validated, and built upon by others in the future._

## Defining Ethical Science
Open, reproducible and FAIR ideas are fundamentally linked to ethics, as these practices ultimately promote more ethical and responsible research by fostering transparency, integrity, and accountability. No matter how efficient or reproducible a study may be, its value is questionable, however, if it causes harm to a group of individuals. Likewise, recognizing biases and risks in our research without providing the necessary resources for replication may undermine the advancement of truly ethical and reliable research.

This section aims to provide a clear definition used in this thesis, ensuring a shared understanding on what I mean by "ethical science" moving forward. Scientific ethics refers to the principles and guidelines that govern the behavior of scientists in their work, and their impact in wider society [@rollin2006Science; @edel2018Science; @menapace2019Scientific]. More broadly, ethics can be seen as a system of moral values and principles that a particular society or community upholds [@gensler2017Ethics]. Ethics can be understood as a core aspect of moral philosophy -an area of study that, among other things, seeks to differentiate between right and wrong. Viewing ethics as the study of what constitutes appropriate or inappropriate behavior emphasizes its connection to action. Therefore, we can see how ethics is relevant when considering the impact of actions and behaviours of scientists throughout the research process. Recognizing this also highlights the fundamental role of moral philosophy and ethics in shaping how research impacts society.

Ethical standards and beliefs evolve over time, and as a result, so do the ethical guidelines that govern scientific research [@joyner2002Evolution; @kim2003Ethical; @macklin2008Standard]. Practices that were once considered acceptable may no longer align with contemporary ethical norms, just as certain current methodologies may be deemed unethical in the future. Either way, throughout history, including current times, there are numerous examples of scientific research leading to significant ethical concerns and harmful consequences [REFERENCES -see also DATA HAZARDS PAPER FOR FURTHER DISCUSSION]. I believe it is important to recognise the heavy weight that science carries, of not only unethical but also oppressive research that has undermined and hurt minoritised groups and individuals throughout time. Unless we recognise where scientific research stands and where it comes from, we cannot stop repeating the same oppressive behaviours that may go unspoken or unrecognised otherwise. 

In a presentation I delivered in 2022, I provided several examples of historical social biases that persist in scientific research, with a focus on Medicine, Neuroscience and Computer Science, including examples of racism, sexism, ableism, and speciesism (Garcia, Sterratt, and Stefan 2022). I define social bias as a systematic and often unconscious prejudice or favoritism toward certain groups over others, based on characteristics such as race, gender, species, or other socially constructed categories. A good example of embedded biases in science is given by Branch et al. (2022) as they eloquently articulate how a desire to quantify and establish hierarchies among organisms was not purely for scientific interest, but that there is extensive evidence in the fact that the roots of evolutionary biology, which serves as a baseline for many other disciplines like neuroscience, are steeped in histories of white-supremacism, eugenics, and scientific racism. They discuss the definition of the “Not-So-Fit”, and how this limits the diverse thought and investigative potential in biology. This is important to recognise for this thesis, as I use hierarchies and models of biology that are based on a historical context of how science has reached it’s current status of knowledge.

## Why is this important?
Thinking about the (un)ethical history and future implications of the research we do can help to uphold accountability and potentially reduce harm [REFERENCES], as we further discuss in DATA HAZARDS CHAPTER. It is the synergy of combining opennes, reproducibility and ethics that create research that shows integrity and enhances credibility. Thinking about reproducibility can in turn help to think how you will share your data, as well as where your own data has come from. Hence, reaching an increased awareness of how your data was sourced and its ethics and potential biases. Working with ethics, philosophy, reproducibility and an openness to discuss the wider context of where our research rests, may add time to the research timeline, but can very much enrich a fuller and more complex understanding of the shortcomings of our research and how to do better moving forward. This is why I believe that continuous reflection on ethical standards is essential, and why I have made a big emphasis on reiterative reflection throughout this PhD. 

As we have seen, creating research that takes into account opennes, reproducibility and ethical considerations offers benefits not only for researchers but for society as a whole. Researchers can directly benefit through increased visibility and citation rates, greater opportunities for collaboration, and reduced waste of time and resources. Additionally, fostering open scientific practices helps build a stronger sense of community, in contrast to the traditionally competitive nature of research. Embedding these considerations into research workflows can enhance the credibility and reliability of scientific findings. Furthermore, reflecting on the broader impact of research promotes accountability and encourages proactive measures to mitigate potential harm. 

Beyond academic advantages, open, ethical, and reproducible research contributes to more equitable knowledge dissemination. When data, methods, and findings are openly shared, barriers to access are reduced, enabling a wider range of researchers, institutions, and communities to engage with and build upon existing work. This can drive innovation, improve public trust in science, and ensure that research benefits can be reaped by as many individuals as possible. 

### A personal account: Where it started 
Before turning to the technical discussion of Data Hazards in the following chapter, I offer a brief personal account of how I came to engage with these issues, alongside a poster I developed to capture the key questions and reflections that have shaped my thinking throughout this PhD.

A common experience in my academic career has been the limited focus on the social dimensions of ethics in the research I conduct. As I became aware that much of the research I conduct is based on biased views of how the world works [REFERENCES], I developed a keen interest in examining the biases within my own work. 

My lived experiences as a woman, immigrant in the UK, and someone from a working-class background have deeply influenced my perspective. At the same time, I also acknowledge the privileges I hold, being perceived as white, European, and a student at a prestigious university. These intersecting experiences of marginalisation and privilege have shaped and motivated my commitment to critically examine implicit biases within society and strive for a more equitable and accessible research process.

My focus on addressing biases in my research made me realise the need for greater reproducibility and accessibility to ensure open, meaningful dialogues, and progress. The initial bias I encountered was the assumption that "there is no need to account for social bias in my research", because one might think my research topic is neutral at first glance: computer simulations of proteins does not scream a need for ethical considerations, mainly because we are not directly using animals, human or non-human. 

In an effort to challenge the initial notion that my project does not require ethical considerations, I began exploring how others were thinking about bias and reproducibility. This led to the creation of a detailed (though wordy) poster, which framed my PhD journey as a series of milestones. I presented this poster at the COMBINE and ICSB 2022 conferences [@romangarcia2022Biasa], where I shared how I address bias and reproducibility in my own work, and encouraged researchers at various stages of their careers to reflect on these aspects in theirs. A common concern raised during these discussions was the additional time and effort required to make research fully reproducible. While it is true that ensuring reproducibility can increase an individual’s workload and extend the research timeline, it ultimately proves more efficient in the long run. By making research accessible, well-documented, and easy to build upon, future researchers are better equipped to continue and expand on existing work, reducing redundancy and advancing scientific progress more effectively.

At each milestone proposed in the poster, I present potential biases that could influence the work, along with tools to either mitigate or highlight them. At the same time, I recognised that for my PhD to be truly ethical, it needed to prioritise both reproducibility and accessibility. To reflect this, I organised the questions I had been grappling with into different sections of the research process. All of this is documented in a publicly available GitHub repository [@romangarcia2022Biasa].

For instance, during the design of the computational model I use, I consider questions such as: Who is my data including or excluding? What assumptions am I making? To what extent am I simplifying? In my research, I work mostly with data from non-human animals. A common assumption in the field is that findings from experiments on mice can be extrapolated to humans. However, it is essential to acknowledge that this remains an assumption, not a certainty. Similarly, my work often involves explaining memory and learning in highly simplified terms, typically focusing on specific molecular mechanisms. Yet, these processes are inherently complex, and we do not fully understand all their nuances. Recognising these limitations is crucial for maintaining scientific integrity and ensuring that findings are interpreted within their appropriate context (see @fig-posterPhDjourney for resources I used). 

These questions around data provenance and the ethical dimensions of my research are explored in greater depth in the next section, where I introduce and explore the Data Hazards framework (see chapter @sec-data-hazards-chapter). This framework encourages researchers to critically assess not only the potential risks and limitations of their work, but also how it might be used in future, and what biases it could be perpetuating. Reflecting on these concerns shaped the design of my computational model and prompted a broader engagement with the ethics and reproducibility of research in neuroscience. This line of thinking ultimately led to the development and publication of the paper included in the following chapter "Data Hazards as an Ethical Toolkit for Neuroscience" [@garcia2025Data]. 

[![Poster about bias and reproducibility, showing research cycle as a journey which starts with design, then data collection, data analysis and final reporting, and compares this through images to growing an apple tree, collecting the apples and then selling them.](22-reproducibility-figures\20221006_poster_phd_journey.jpg){#fig-posterPhDjourney fig-pos="h" width=500}](https://github.com/Susana465/Bias-and-Reproducibility-Poster/blob/main/20221006_poster_phd_journey.jpg)

