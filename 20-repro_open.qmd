---
bibliography: [references.bib]
---
# Creating an open, reproducible and ethics-focused PhD
Intro paragraph

## Reproducibility Crisis
In recent decades, scientific research has accelerated significantly, with a "publish or perish" culture becoming deeply ingrained in the academic community’s consciousness [@angell1986Publish; @rawat2014Publish]. Frequent publication is widely regarded as a powerful tool for scholars to showcase their expertise and gain recognition among their peers. Likewise, there is a generalised view that this keeps academics constantly engaged with relevant knowledge works in their fields of expertise. Successfully published research not only enhances an individual’s reputation but also raises the profile of their institution, often attracting greater funding opportunities. Moreover, academic institutions and universities commonly use the number of publications credited to a researcher as a key indicator of their competence and career progression [@york2015Defining; @aksnes2019Citations;@elbanna2023Publish]. In this view, a strong publication record can be beneficial, opening up opportunities for researchers and their institutions. 

However, this mindset exerts significant pressure on researchers to continuously publish in order to secure funding and advance their careers [REFERENCES]. What is more,  the "publish or perish" culture is deeply embedded in colonial Global North views, where academic knowledge produced in wealthier countries is often regarded as superior [@amutuhaire2022Reality]. This reinforces Eurocentric biases, marginalises and even dismisses the realities and contributions of the Global South [REFERENCES]. Similarly, the expectation for a high volume of publications impacts disproportionately low-paid academics in resource-constrained countries, who can face a variety of structural barriers, including limited grant funding [REFERENCE] and inaccessible academic journal costs [REFERENCE] among many other reasons [REFERENCES]. 

In the culture of continuously publishing and publishing fast, those who are already at a disadvantage due to systemic bias and oppression are the ones who continue to lose the most. For example, research has shown that minoritised communities —whether this be due to race, gender, sexual orientation, or other factors— are underrepresented in prestigious journals, partly due to implicit biases and the dominance of Western academic frameworks [REFERENCES]. Consequently, the "publish or perish" culture overwhelmingly benefits scholars in wealthier countries while deepening global inequalities in knowledge production.

Moreover, this pressure to publish (predominantly statistically significant results), is closely tied to what is known as publication bias. In published academic research, publication bias occurs when the outcome of an experiment or research study biases the decision to publish or otherwise distribute it. This typically results in a preference for publishing significant or positive results, disrupting the balance of findings and favouring those that are deemed novel or noteworthy. Whilst negative or inconclusive results are rarely published or considered less worthy of publication [REFERENCES].

https://onlinelibrary.wiley.com/doi/full/10.1111/emre.12618?casa_token=j3R2YYkfgVsAAAAA%3A8LuIsxYdzSA6FiWCaHMq2Ox_nalGjhqFqnoXbCo9p8RApG0iDTjrhPCb2bu0vYRY9mW370rjipSCW4E

https://www.tandfonline.com/doi/full/10.1080/01425692.2025.2454315?casa_token=F2Ti3XPhKU8AAAAA%3AiWYVo5oi-M2kwXK9Uaxj7GsFDO4kkcYv2h7RvkzYNO12puyFDCwMLC0zzF_dm5VLW2WJMB43ZRE1gg

Examining this bias towards positive results reveals substantial evidence of a troubling trend: an overrepresentation of false-positive findings in the scientific literature [@esarey2016Measuring; @hartgerink2017Too; @stahl2018Fact; @schneck2023Are]. This bias towards positive results has several concerning and wasteful consequences, including, but not restricted to: a significant number of valid negative results remaining unpublished, which excludes critical findings from the scientific record. This means that other research teams, unaware of these unpublished negative results, may continue to unknowingly test the same hypotheses (which may actually be false) until, by chance or artifact, a positive result is obtained. These chance positive results are then published, as they align with the preference for significant findings, even when substantial and more definitive contradictory evidence may exist in contrast to it [@carlson2012Understanding]. What is more, this underreporting of negative results introduces bias into meta-analysis, which consequently misinforms researchers, doctors, policymakers and the public in general. Additionally, more resources are wasted on already disputed research that remains unpublished and therefore unavailable to the scientific community [@kicinski2013Publication; @kicinski2014How; @aert2019Publication; @page2019Assessing]. 

This cycle sustains an error-prone body of scientific literature, undermining the reliability of published research. Hand in hand with this publication bias, comes something now known as a _Reproducibility Crisis_, where it is becoming increasingly apparent that making "fast-science" is associated with a lack of being able to reproduce research [REFERENCE]. The reproducibility crisis has been in the rise for the past few decades as we uncover the fact that much of the research that is published fails to be reproduced by others. To give some examples, a survey of 1576 scientists published in Nature (Baker, 2016) reported that over 70% of the participants failed to reproduce others’ experiments and over 50% failed to reproduce their own results. Similarly, @tiwari2021ReproducibilitySystemsBiology assessed the reproducibility of 455 mathematical models in systems biology and found that about 50% of published models were not reproducible either due to incorrect or missing information in the manuscript [REFERENCE].

Much of the criticism surrounding the reproducibility crisis centres on statistical methods and research practices [REFERENCES]. Problematic scientific behaviours, such as HARKing (Hypothesising After the Results are Known), p-hacking (manipulating data analysis to achieve statistical significance), and selectively reporting only positive outcomes, as discussed above, have been identified as major contributors to irreproducibility [REFERENCES]. Indeed, this reliance on p-values, provided they meet the conventional threshold of statistical significance (typically p < 0.05), can sometimes lead to studies being treated as definitive evidence, even when their findings are not robust or reproducible [9–11]. 

The list of factors contributing to irreproducibility is extensive, and is not restricted to statistical methods. A lack of access to raw data or, in some cases, outright data fabrication also plays a role [REFERENCE]. Ambiguities in experimental procedures, materials, and data processing steps further undermine research reliability [REFERENCES]. On a broader, more systemic level, there's an important discussion regarding how current academic systems often prioritize novelty and statistically significant findings, as research funding is more likely to be secured and promoted when the outcomes are more profitable [REFERENCES]. Although these behaviours fall under scientific misconduct [REFERENCE: https://publications.parliament.uk/pa/cm5803/cmselect/cmsctech/101/report.html] and are not considered acceptable scientific practice, they continue to occur as th mindset of publish or perish continues to embed itself in how scientists work.

Interestingly, a large-scale survey of nearly 6,000 academic psychologists (with 2,155 responses) assessed self-reported engagement in some of these questionable research practices known to introduce bias into research findings. Notably, respondents often justified their own use of these practices while simultaneously viewing them as unacceptable when used by others (p. 530). This is just one example, and it is worth noting that scientific standards and methodologies continuously evolve, meaning that questionable research practices are not static. Methods once tolerated, or even considered standard, may now be recognised as problematic, reflecting our ability to reassess and refine ethical and methodological frameworks. Encouragingly, awareness of the reproducibility crisis is growing, and there is a gradual shift towards promoting open and reproducible research practices [@umbach2024Open, @nature2018Challenges]. Before we explore the work being done to promote science as more open, reproducible, and ethical, it’s important to first agree on what we mean exactly by these terms in this thesis.

## Defining Reproducibility
There is a long history of the terms reproducibility and replicability being used interchangeably, or their meanings being swapped depending on the field of study [@claerbout1992ElectronicDocumentsGive], [@ivie2018ReproducibilityScientificComputing],[@plesser2018ReproducibilityVsReplicability]. For example, a review on the usage of reproducible/replicable meanings [@barba2018TerminologiesReproducibleResearch] showed that most papers and disciplines use the terminology as defined by Claerbout and Karrenbach, whereas microbiology, immunology and computer science tend to follow the Associtation for Computing Machinery use of reproducibility and replication given by [@ivie2018ReproducibilityScientificComputing]. In political science and economics literature, both terms are used interchangeably. So this quickly shows how having a lack of agreement on such definitions can add even more confusion to the mix. 

In this PhD, we use the definition used by [@turingwaycommunity2019TuringWayHandbook], where reproducible research is understood as "_work that can be independently recreated from the same data and the same code that the original team used_". Reproducible, replicable, robust and generalisable have different meanings as described in the table below.

![How the Turing Way defines reproducible research.](22-reproducibility-figures/repro_definintions.PNG){#fig-repro-definitions width=65%}

- **Reproducible research**: is obtained when same analysis is performed on the same data, to produce the same results.

- **Replicable research**: refers to conducting the same analysis on different datasets, resulting in qualitatively similar outcomes. 

- **Robust research**: entails subjecting the same dataset to different analysis workflows to address the same research question, such as employing distinct pipelines in R and Python. Robustness demonstrates that findings can remain consistent regardless of different methods used for analysis, indicating validity and resilience to various factors like changes in conditions or methods (such as different programming languages).

- **Generalisable research**: refers to findings or conclusions that can be applied beyond the specific context in which they were derived. It indicates that the results are not limited to a particular dataset, methodology, or experimental setup, but instead can be extended to broader populations, situations, or conditions. By combining replicable and robust research, we can obtain more generalisable results.

## Defining Open Science
Open science is an approach to the scientific process that promotes cooperative work and new ways of diffusing knowledge accessible to everyone, without barriers such as paywalls or restrictions on the use of research outputs. By making research more accessible and transparent, open science seeks to enable more efficient scientific progress, enhance reproducibility, and increase the societal impact of research findings. Whilst there are varying ways of defining what this "opennes" means for different contexts [REFERENCES], a definition provided by @vicente-saez2018Open gives an overall good idea of what open science is referring to: "_Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks, [it] helps the scientific community, the business world, political actors, and citizens [...] and stimulates an open debate about the social, economic, and human added value of this phenomenon._" Additionally, the United Nations Educational, Scientific and Cultural Organization (UNESCO) promote the following message in their Recommendation on Open Science: "_By promoting science that is more accessible, inclusive and transparent, open science furthers the right of everyone to share in scientific advancement and its benefits as stated in Article 27.1 of the Universal Declaration of Human Rights_".

Open Science is an overall movement that includes related concepts such as Open Data, Open Source [@Open] and Open Access [@What] among others [@Opena; @parsons2022Communitysourced; @turingwaycommunity2019TuringWayHandbook; @Openb]. I provide a few definitions that are a of particular relevance to this thesis: **Open Data** refers to the openness and accessibility of data. It involves thinking about data sharing, privacy, and protection, as well as considerations like consent and the nuances of handling sensitive data. In the context of computational projects like this PhD, **Open Source** applies to both software (e.g., programs and applications used) and hardware (e.g., types of machines involved) that are publicly accessible, allowing anyone to view, modify, use, and distribute them, usually under **open licensing** [@Guide; @kreutzer2014Open]. **Open Access** refers to how freely available research content is. There are different ways to achieve this. Some times, open access might involve paying an Article Processing Charge to a journal, which then publishes the final version of the article under an open license [@borrego2023Article], making it permanently free to access online (this is known as Gold Open Access). Sometimes, it may involve self-archiving a version of the research, often alongside preprints, allowing public access without direct journal fees (this is known as Green Open Access) [@bjork2014Anatomy].

In order to achieve Open Science, the research process should:

1.	**Be publicly available**: It's hard to benefit from knowledge hidden behind barriers like passwords and paywalls.

2.	**Be reusable**: Research outputs should be licensed adequately, informing potential users of any restrictions on reuse.

3.	**Be transparent**: With appropriate metadata to provide clear statements of how research output was produced and what it contains.

Additionally, Open Science and its various elements are directly related with the broader concept of **Open Scholarship** [@scanlon2014Scholarship; @tennant2019Foundations]. Open Scholarship promotes transparency and accessibility in teaching, learning, research, and academia [@emeryLibGuides]. More importantly, Open Scholarship emphasizes equity, diversity, and inclusion, ensuring that knowledge is openly available to everyone, regardless of ethnicity, gender, sexual orientation, or other protected characteristics.

It is worth noting that Open Science does not mean “sharing absolutely everything”. Many fields of science involve working with sensitive personal data, with medical research being the most obvious example, where data is not to be widely shared. Likewise, privacy and data protection, as well as consent, and national and commercially sensitive data can be some of the most common examples of when data cannot always be open [@regulation2016RegulationEU2016]. If access to data needs to be restricted due to security reasons, however, the justification for this should be made clear. Free access to and subsequent use of data is of significant value to society and the economy. The concept of Open Science views that data should, therefore, be open by default and only as closed as necessary.

## FAIR principles for open and reproducible science
Weaved in with the topics already discussed, are the FAIR principles for scientific data management. These principles were created as a guideline to develop and collectively support a clear and measurable set of principles that allow for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability of digital assets, to ultimately support more reproducible research [@wilkinson2016FAIRa]. FAIR principles, therefore, serve as a valuable framework for conducting research with integrity:

- **Findable** data is well-described with rich metadata, making it easier to locate by both humans and machines. 
- **Accessible** data is stored in a way that ensures users can retrieve it with clear licensing and open protocols. 
- **Interoperable** data is formatted using standardised vocabularies and structures, enabling seamless integration across different applications or workflows. 
- **Reusable** data is well-documented, with detailed provenance and clear usage rights, supporting reproducibility and future research.

The advantages of applying FAIR principles become apparent when we understand each principle. For example, when data is more easy to find, more accessible and readable, discoverability naturally improves [@cole2017Using; @kremen2018Improving; @kim2024Findability]. Beyond this, and more specifically linked to computational projects such as this PhD, FAIR practices can lead to more efficient code, minimising time spent on things like model retraining/rewriting, and reducing redundant data generation and storage [@hasselbring2019FAIR; @borrego2023Article]. Likewise, the interoperability aspect of FAIR principles enables researchers to integrate datasets from various sources, fostering new insights and innovative problem-solving approaches. Ultimately application of FAIR principles can result in lowering carbon footprint of computer simulations as less time and resources are wasted trying to run models that lacks useful information on how to run it [@lannelongue2023GREENER].

It is not uncommon for scientific code and workflows to originate within small, specialised research groups, often resulting in code that functions as a 'black box' to external researchers and developers. As this process unfolds and mixes with the fast-paced mindset mentioned above, if a workflow and materials are not clear from the beginning, future researchers may rioritise immediate functionality over long-term code quality, often due to constraints such as tight deadlines, limited resources, or evolving project requirements, causing something known as technical debt where the long term costs of workflow opacity costs human effort, money and time. One of the goals of this thesis project is to provide a workflow that enables reusability at initial creation and at the time of reuse, so that technical debt is decreased and the barriers for driving community interactions and innovations are lowered as a result.

In order to make research FAIR, there are numerous resources out there [REFERENCES] that offer guidance on how to do so. Generally, it is recommended to think about this at the different stages of the research life cycle, as an iterative process. In the [REFERENCE CHAPTER HOW I MAKE THIS PHD REPRODUCIBLE] I will dive deeper as to how these principles are thought about in this PhD and how they have been applied throughout the different stages of the research carried out here. 

## How are these principles applied throughout this PhD?
While extensive literature and tutorials exist on best practices for reproducibility, the following provides a personal account of the tools and methods I have employed, along with their practical benefits in creating a research model that others can reproduce. This not only demonstrates the practical implementation of these practices but also showcases this PhD as an example of working towards a reproducible, open, and ethics-focused PhD in computational neuroscience. I reflect on how I have engaged with these principles, adopted the most relevant practices for this project, and integrated them into my research. 

In order to ensure compliance with the FAIR principles, I have referenced checklists such as [REFERENCES] as a guide to applying these standards. The following sections demonstrate how the FAIR principles have been integrated into this PhD. Each section outlines specific practices that I have used in order to make this research more open, transparent, and reproducible. These efforts align closely with best practices for reproducible research proposed by TTW [REFERENCE].

### Findable
- The model and thesis are available in GitHub repositories, where people can access and find data, with appropriate README files explaining how each repository works and what it contains.
- The dataset will be available with a DOI upon finalization [insert DOI once ready].
- Presentations, workshops, and papers are available with persistent identifiers where applicable [REFERENCE REPOS AND DOIs].
- Metadata used in the model follows human-readable naming conventions while still being domain-specific within neuroscience. Although some terminology is niche, notations and explanations are provided [see code link].

### Accessible
- The models created during this PhD project are open and accessible through GitHub, with proper documentation.
- Data is stored in [chosen database], ensuring long-term access.
- README files provide clear guidance on protocols, which are open, free, and implementable, provided the necessary software and hardware are available.

### Interoperable
- This aspect is particularly relevant to this project, as the model needs to interface with multiple software tools (e.g., BioNetGen, MCell, Python) [see CHAPTER-REFERENCE]. By structuring the data and code to be compatible across these platforms, I ensure that the model can be integrated with future datasets and incorporated into diverse workflows for analysis, storage, and processing.
- The scripts written in Python utilize libraries that facilitate cross-software integration, making the model adaptable [see CHAPTER-REFERENCE].
- Ideally, a vocabulary that is interoperable includes terms and concepts that have e globally unique and persistent identifiers. The nomenclature in this project tries to follow similar naming of molecules but there are slight variances in naming as the model is not exactly the same and molecules functions and behaviours are not the same as other models. However, using BioNetGen Language and Python allows for a formal, accessible, shared and broadly applicable language for knowledge representation.

### Reusable
- I have provided a detailed description of the model and its components, ensuring that (meta)data are richly annotated with relevant attributes. Specifically, in the model-creation section, I outline the conditions under which the data was generated, including kinetic rates, molecule reaction rule names, and their sources or calculation methods.
- Where appropriate (e.g., GitHub repositories or data repositories), I have explicitly licensed the data and stated usage rights, making it clear how others can build upon this work.
- The model includes comprehensive provenance details [see table of reactions with references], documenting data origins, derivation methods, and any transformations applied. 

https://www.data.cam.ac.uk/data-management-guide/sharing-your-data

https://journalologytraining.ca/topic/checklist-to-ensure-your-data-support-the-fair-principles/

### Version Control 
Version control is a pillar of most, if not all, guides for reproducible research. Version control is a method for tracking and managing changes to files or projects over time. It allows you and your collaborators to monitor modifications, review past edits, and restore previous versions if needed. Using version control helps maintain a relatively organized and traceable record of updates across different stages of development. This is extremely helpful for times when I needed to check back on when and where I did a specific change to my work, or when working collaboratively to know who wrote what and when. In terms of reproducibility, this means that version control helps with having clear provenance of information [REFERENCE]. As a result of using version control, I have made my life easier (and potentially others in the future too) as I could track what version of the code and data produced specific outputs, for example. And as datasets grow larger and more complex, having a solid version control workflow feels like doing your future self a huge favor (because you are).

There are various ways to implement version control [REFERENCES], but I primarily use Git and GitHub —partly because they are widely adopted and partly because they were the tools introduced in the version control courses I attended. With Git, each version update (or "commit") can include a message describing the changes made, such as "changed value x to y." This has made it much easier to track modifications over time and follow the evolution of this project. Version control is therefore, especially useful when sharing analyses, as it ensures transparency, reproducibility, and auditability -key aspects of good scientific practice.

branches anf quarto

- there are different possibilities to use version control, i use github
- benefits
- it can be scary sometimes

### Clear and Reusable Code
- clear comments
- clear documentation of what software, hardware and versions I used
- readme files
- environments for future reuse

### Code Testing
- prepare for errors
- validate step by step (learn to debug)
- use of a mock/dummie test model (as main one is too convoluted to follow errors sometimes). ABC model
- things like unit testing, system testing, runtime testing, mention too.

As someone with a heavier background of biology - I never learnt how to code in my undergrad - I must admit I don't find coding intuitive. Since my undergraduate degree, I did a 9 months masters into computer science, and I have completed various courses on good computer science practices during my PhD. And although of course these things have been helpful, I think most of the steps I have taken towards making sustainable code has

asses how successful/where things could be improved.

--

how have I made my PhD reproducible and followed FAIR principles? is what I am trying to get at, so that people can not only see how it could be done, but maybe use it as inspiration to do so as well. there is very extensive literature and tutorials out there on how to do all of these things, but I will provide below a personal account of what I have used and how it has been useful to create a model that can reproduced by other people. Later, on the social ethics of it, I discuss data hazards and application to this PhD. 

I guess it makes sense to divide how ive done things into various sections, although the whole process has included many back and forth and continuous learning throughout. 

- make an environment, repository for code, use github for version control, open publication, trying my best to follow good software practices, and then there is the use quarto for thesis write up. 

## Defining Ethical Science
Open, reproducible and FAIR ideas are fundamentally linked to ethics, as these practices ultimately promote more ethical and responsible research by fostering transparency, integrity, and accountability. Likewise, regardless of how efficient or reproducible a research outcome is, its value is questionable if it causes harm to a group of individuals. Likewise, if a project acknowledges potential biases and risks in its data but fails to provide sufficient resources for others to replicate the research, can we truly call it progress?

This section does not aim to address the extensive literature on ethics [REFERENCES], but rather to provide a clear definition, ensuring a shared understanding on what I mean by "ethical science" moving forward. Scientific ethics refers to the principles and guidelines that govern the behavior of scientists in their work, and their impact in wider society. More broadly, ethics can be seen as a system of moral values and principles that a particular society or community upholds. Ethics can be understood as a core aspect of moral philosophy -an area of study that, among other things, seeks to differentiate between right and wrong. Viewing ethics as the study of what constitutes appropriate or inappropriate behavior emphasizes its connection to action. Therefore, we can see how ethics is relevant when considering the impact of actions and behaviours of scientists throughout the research process. Recognizing this also highlights the fundamental role of moral philosophy and ethics in shaping how research impacts society.

Ethical standards and beliefs evolve over time, and as a result, so do the ethical guidelines that govern scientific research. Practices that were once considered acceptable may no longer align with contemporary ethical norms, just as certain current methodologies may be deemed unethical in the future. Either way, throughout history, including current times, there are numerous examples of scientific research leading to significant ethical concerns and harmful consequences [REFERENCES -see also DATA HAZARDS PAPER FOR FURTHER DISCUSSION]. I believe it is important to recognise the heavy weight that science carries, of not only unethical but also oppressive research that has undermined and hurt minoritised groups and individuals throughout time. Unless we recognise where scientific research stands and where it comes from, we cannot stop repeating the same oppressive behaviours that may go unspoken or unrecognised otherwise. 

In a presentation I delivered in 2022, I provided several examples of historical social biases that persist in scientific research, with a focus on Medicine, Neuroscience and Computer Science, including examples of racism, sexism, ableism, and speciesism (Garcia, Sterratt, and Stefan 2022). I define social bias as a systematic and often unconscious prejudice or favoritism toward certain groups over others, based on characteristics such as race, gender, species, or other socially constructed categories. A good example of embedded biases in science is given by Branch et al. (2022) as they eloquently articulate how a desire to quantify and establish hierarchies among organisms was not purely for scientific interest, but that there is extensive evidence in the fact that the roots of evolutionary biology, which serves as a baseline for many other disciplines like neuroscience, are steeped in histories of white-supremacism, eugenics, and scientific racism. They discuss the definition of the “Not-So-Fit”, and how this limits the diverse thought and investigative potential in biology. This is important to recognise for this thesis, as I use hierarchies and models of biology that are based on a historical context of how science has reached it’s current status of knowledge.

## Why is this important?
Thinking about the (un)ethical history and future implications of the research we do can help to uphold accountability and potentially reduce harm [REFERENCES], as we further discuss in DATA HAZARDS CHAPTER. It is the synergy of combining opennes, reproducibility and ethics that create research that shows integrity and enhances credibility. Thinking about reproducibility can in turn help to think how you will share your data, as well as where your own data has come from. Hence, reaching an increased awareness of how your data was sourced and its ethics and potential biases. Working with ethics, philosophy, reproducibility and an openness to discuss the wider context of where our research rests, may add time to the research timeline, but can very much enrich a fuller and more complex understanding of the shortcomings of our research and how to do better moving forward. This is why I believe that continuous reflection on ethical standards is essential, and why I have made a big emphasis on reiterative reflection throughout this PhD. 

As we have seen, creating research that takes into account opennes, reproducibility and ethical considerations offers benefits not only for researchers but for society as a whole. Researchers can directly benefit through increased visibility and citation rates, greater opportunities for collaboration, and reduced waste of time and resources. Additionally, fostering open scientific practices helps build a stronger sense of community, in contrast to the traditionally competitive nature of research. Embedding these considerations into research workflows can enhance the credibility and reliability of scientific findings. Furthermore, reflecting on the broader impact of research promotes accountability and encourages proactive measures to mitigate potential harm. 

Beyond academic advantages, open, ethical, and reproducible research contributes to more equitable knowledge dissemination. When data, methods, and findings are openly shared, barriers to access are reduced, enabling a wider range of researchers, institutions, and communities to engage with and build upon existing work. This can drive innovation, improve public trust in science, and ensure that research benefits can be reaped by as many individuals as possible. 

### Getting started: Bias and reproducibility in a computational neurobiology PhD's journey
A common experience in my academic career has been the limited focus on the social dimensions of ethics in the research I conduct. Over time, I developed a keen interest in examining the biases within my own work, as I became aware that a lot of the research I do is based on biased views of how the world works (although it is almost -if not fully- impossible to have no bias) [REFERENCES]. This led me to the realization that in order to address and acknowledge any biases in my research, it also needed to be more reproducible and accessible, so that conversations are open and hold integrity. The initial bias I encountered was the assumption that "there is no need to account for social bias in my research", because one might think my research topic is neutral at first glance: computer simulations of proteins does not scream a need for ethical considerations, mainly because we are not directly using animals, human or non-human. 

In an effort to challenge the initial notion that my project does not require ethical considerations, I began exploring how others were thinking about bias and reproducibility. This led to the creation of a detailed (though wordy) poster, which framed my PhD journey as a series of milestones. I created this poster to share with a conference of scientists at various stages of their research how I address bias and reproducibility in my own work, and to encourage them to consider these aspects in their own research as well [REFERENCES]. At each milestone, I proposed potential biases that could influence the work, along with tools to either mitigate or highlight them. At the same time, I recognised that for my PhD to be truly ethical, it needed to prioritise both reproducibility and accessibility. To reflect this, I organised the questions I had been grappling with into different sections of the research process. All of this is documented in a publicly available GitHub repository [REFERENCE: https://github.com/Susana465/Bias-and-Reproducibility-Poster].

I structured the PhD journey into four key stages: design, data collection, data analysis, and results. Within each stage, I identified and documented critical questions that I ask myself—questions that may also be useful for others. It is important to note that many of these questions arise iteratively throughout the entire research process.

For instance, during the design of the computational model I use, I consider questions such as: Who is my data including or excluding? What assumptions am I making? To what extent am I simplifying? In my research, I work mostly with data from non-human animals. A common assumption in the field is that findings from experiments on mice can be extrapolated to humans. However, it is essential to acknowledge that this remains an assumption, not a certainty. Similarly, my work often involves explaining memory and learning in highly simplified terms, typically focusing on specific molecular mechanisms. Yet, these processes are inherently complex, and we do not fully understand all their nuances. Recognising these limitations is crucial for maintaining scientific integrity and ensuring that findings are interpreted within their appropriate context. When it comes to reproducibility, version control, code literacy, and early planning for data storage and sharing have been invaluable throughout my research. Establishing these practices from the beginning has greatly enhanced the transparency and reliability of my work (see @fig-posterPhDjourney for resources I used). 

When collecting, analysing and reporting data, it has been important for me to consider where data originates and the potential biases involved at each stage. For instance, data are not just numbers —they often come from living beings that may have been used for experimentation, raising ethical considerations. Acknowledging this is essential in ensuring responsible and thoughtful research practices. Moreover, even with an extremely detailed description of the methods and workflows used to achieve the final result, reproducing it is often challenging [REFERENCES]. This is where concepts like Data Hazards [SEE CHAPTER] become relevant, encouraging researchers to critically assess potential risks and limitations. As well as considering how one’s research will be used in the future and what biases it may be contributing to. 

(I feel like I need to be a lot more specific here but maybe this is for dicussion not for here? struggling to clarify and be super specific/lay it out that way)

[![Poster about bias and reproducibility, showing research cycle as a journey which starts with design, then data collection, data analysis and final reporting, and compares this through images to growing an apple tree, collecting the apples and then selling them.](22-reproducibility-figures\20221006_poster_phd_journey.jpg){#fig-posterPhDjourney fig-pos="h" width=500}](https://github.com/Susana465/Bias-and-Reproducibility-Poster/blob/main/20221006_poster_phd_journey.jpg)

I presented this poster at the COMBINE and ICSB 2022 conferences [REFERENCES], where a common concern raised was the time required to make research fully reproducible. While it is true that ensuring reproducibility adds to an individual’s workload and extends the research timeline, it ultimately saves time in the long run [REFERENCES]. By making research accessible, well-documented, and easy to build upon, future researchers can efficiently continue and expand upon previous work, reducing redundancy and improving overall scientific progress.

### Diving deeper: Open Seeds project - Ethical Standards and Reproducibility of computer models in Neurobiology
You may notice that this project shares a similar title with the poster above, this is intentional as the two are closely connected. This project developed in collaboration with the Open Seeds group (formerly known as Open Life Science), emerging as a direct extension of the ideas presented in the above-mentioned poster. During my time with Open Seeds [REFERENCE] and the Turing Enrichment placement [@atiTuring, @merritt2023Enrichment], I worked on this project with the aim of creating a practical guide and case study to support researchers in developing more reproducible and ethical research. Using my PhD journey as an example, this work led to the paper "Data Hazards as an Ethical Toolkit for Neuroscience," [LINK TO SECTION IN THESIS] in which my co-authors and I outline a step-by-step approach to integrating ethical considerations and practical strategies for enhancing research integrity. 

Overall, this project was an excellent exercise where I honed in my knowledge on licensing, version control and GitHub, all of which I have applied as can be seen through the available GitHub repositories of the work I have made [REFERENNCES]. Moreover, it was an opportunity to brainstorm ideas of how I could offer a case-study example of how to create a PhD that looks at reproducibility and ethics as part of the process, not as an add-on [@fig-opencanvas]. 

![Following Mozilla Open Leadership Training, this was some brainstorming using Open Canvas further define the project's goals, strategy, and needs for resources [@mozillaopenleadershipOpen]](22-reproducibility-figures\opencanvas.PNG){#fig-opencanvas}

Ultimately, this Open Seeds project led to further applications of ethical and reproducibility practices in my PhD, including the development of the Data Hazards chapter publication in The Turing Way book, and organization of a one-day hybrid symposium, as well as multiple paper publications [links and titles REFERENCES], including the next chapter in this thesis, where I use my PhD as a case study to showcase how Data Hazards can be applied to a computational neuroscience project. 
