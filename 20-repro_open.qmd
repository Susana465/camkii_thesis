---
bibliography: [references.bib]
---
# Creating an open, reproducible and ethics-focused PhD
Intro paragraph

## Reproducibility Crisis
Over the past few decades, scientific research has become increasingly fast-paced, with a "publish or perish" culture firmly embedding itself in the scientific community’s consciousness [REFERENCES]. This mindset exerts significant pressure on researchers to continuously publish in order to secure funding and advance their careers [REFERENCES]. This pressure to publish (predominantly statistically significant results), is closely tied to what is known as publication bias. In published academic research, publication bias occurs when the outcome of an experiment or research study biases the decision to publish or otherwise distribute it. This typically results in a preference for publishing significant or positive results, disrupting the balance of findings and favouring those that are deemed novel or noteworthy. Whilst negative or inconclusive results are rarely published or considered less worthy of publication [REFERENCES].

Examining this bias towards positive results reveals substantial evidence of a troubling trend: an overrepresentation of false-positive findings in the scientific literature. This bias towards positive results has several concerning and wasteful consequences, including, but not restricted to: a significant number of valid negative results remaining unpublished, which excludes critical findings from the scientific record. This means that other research teams, unaware of these unpublished negative results, may continue to unknowingly test the same hypotheses (which may actually be false) until, by chance or artifact, a positive result is obtained. These chance positive results are then published (as they align with the preference for significant findings), even when substantial and more definitive contradictory evidence may exist in contrast to it [REFERENCE]. What is more, this underreporting of negative results introduces bias into meta-analysis, which consequently misinforms researchers, doctors, policymakers and the public in general. Additionally, more resources are wasted on already disputed research that remains unpublished and therefore unavailable to the scientific community. 

This cycle sustains an error-prone body of scientific literature, undermining the reliability of published research. Hand in hand with this publication bias, comes something now known as a _Reproducibility Crisis_, where it is becoming increasingly apparent that making "fast-science" is associated with a lack of being able to reproduce research [REFERENCE]. While one may not directly cause the other, they are closely intertwined. The reproducibility crisis has been in the rise for the past few decades as we uncover the fact that much of the research that is published fails to be reproduced by others. To give some examples, a survey of 1576 scientists published in Nature (Baker, 2016) reported that over 70% of the participants failed to reproduce others’ experiments and over 50% failed to reproduce their own results. Similarly, @tiwari2021ReproducibilitySystemsBiology assessed the reproducibility of 455 mathematical models in systems biology and found that about 50% of published models were not reproducible either due to incorrect or missing information in the manuscript [REFERENCE].

Much of the criticism surrounding the reproducibility crisis centres on statistical methods and research practices [REFERENCES]. Problematic scientific behaviours, such as HARKing (Hypothesising After the Results are Known), p-hacking (manipulating data analysis to achieve statistical significance), and selectively reporting only positive outcomes, as discussed above, have been identified as major contributors to irreproducibility [REFERENCES]. Indeed, this reliance on p-values, provided they meet the conventional threshold of statistical significance (typically p < 0.05), can sometimes lead to studies being treated as definitive evidence, even when their findings are not robust or reproducible [9–11]. 

The list of factors contributing to irreproducibility is extensive, and is not restricted to statistical methods. A lack of access to raw data or, in some cases, outright data fabrication also plays a role [REFERENCE]. Ambiguities in experimental procedures, materials, and data processing steps further undermine research reliability [REFERENCES]. On a broader, more systemic level, there's an important discussion regarding how current academic systems often prioritize novelty and statistically significant findings, as research funding is more likely to be secured and promoted when the outcomes are more profitable [REFERENCES]. Although these behaviours fall under scientific misconduct [REFERENCE: https://publications.parliament.uk/pa/cm5803/cmselect/cmsctech/101/report.html] and are not considered acceptable scientific practice, they continue to occur.

Interestingly, a large-scale survey of nearly 6,000 academic psychologists (with 2,155 responses) assessed self-reported engagement in some of these questionable research practices known to introduce bias into research findings. Notably, respondents often justified their own use of these practices while simultaneously viewing them as unacceptable when used by others. As the authors put it, “Respondents considered these behaviors to be defensible when they engaged in them [...] but considered them indefensible overall” (p. 530). This is just one example, and it is worth noting that scientific standards and methodologies continuously evolve, meaning that questionable research practices are not static. Methods once tolerated, or even considered standard, may now be recognised as problematic, reflecting our ability to reassess and refine ethical and methodological frameworks. Encouragingly, awareness of the reproducibility crisis is growing, and there is a gradual shift towards promoting open and reproducible research practices [@umbach2024Open, @nature2018Challenges]. Before we explore the work being done to promote science as more open, reproducible, and ethical, it’s important to first agree on what we mean by these terms in this thesis.

## Defining Reproducibility
There is a long history of the terms reproducibility and replicability being used interchangeably, or their meanings being swapped depending on the field of study [@claerbout1992ElectronicDocumentsGive], [@ivie2018ReproducibilityScientificComputing],[@plesser2018ReproducibilityVsReplicability]. For example, a review on the usage of reproducible/replicable meanings [@barba2018TerminologiesReproducibleResearch] showed that most papers and disciplines use the terminology as defined by Claerbout and Karrenbach, whereas microbiology, immunology and computer science tend to follow the Associtation for Computing Machinery use of reproducibility and replication given by [@ivie2018ReproducibilityScientificComputing]. In political science and economics literature, both terms are used interchangeably. So this quickly shows how having a lack of agreement on such definitions can add even more confusion to the mix. 

In this PhD, we use the definition used by [@turingwaycommunity2019TuringWayHandbook], where reproducible research is understood as "_work that can be independently recreated from the same data and the same code that the original team used_". Reproducible, replicable, robust and generalisable have different meanings as described in the table below.

![How the Turing Way defines reproducible research.](22-reproducibility-figures/repro_definintions.PNG){#fig-repro-definitions width=65%}

- **Reproducible research**: is obtained when same analysis is performed on the same data, to produce the same results.

- **Replicable research**: refers to conducting the same analysis on different datasets, resulting in qualitatively similar outcomes. 

- **Robust research**: entails subjecting the same dataset to different analysis workflows to address the same research question, such as employing distinct pipelines in R and Python. Robustness demonstrates that findings can remain consistent regardless of different methods used for analysis, indicating validity and resilience to various factors like changes in conditions or methods (such as different programming languages).

- **Generalisable research**: refers to findings or conclusions that can be applied beyond the specific context in which they were derived. It indicates that the results are not limited to a particular dataset, methodology, or experimental setup, but instead can be extended to broader populations, situations, or conditions. By combining replicable and robust research, we can obtain more generalisable results. Generalisability is important as it allows us to make inferences for bigger groups of datasets, for example a target population, by only studying a part of it (the sample).

## Defining Open Science
Open science is an approach to the scientific process that promotes cooperative work and new ways of diffusing knowledge accessible to everyone, without barriers such as paywalls or restrictions on use. By making research more accessible and transparent, open science seeks to accelerate scientific progress, enhance reproducibility, and increase the societal impact of research findings. An whilst there are varying ways of defining what this "opennes" means for different contexts [REFERENCES], a definition provided by @vicente-saez2018Open gives an overall good idea of what open science is referring to: "_Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks [it] helps the scientific community, the business world, political actors, and citizens [...] and stimulates an open debate about the social, economic, and human added value of this phenomenon._" Additionally, The United Nations Educational, Scientific and Cultural Organization (UNESCO) promote the following message in their Recommendation on Open Science: "_By promoting science that is more accessible, inclusive and transparent, open science furthers the right of everyone to share in scientific advancement and its benefits as stated in Article 27.1 of the Universal Declaration of Human Rights_".

Open Science overlaps with other concepts such as Open Data [REFEREENCE DEFINTION], Open Source Software and/or Hardware [REFEREENCE DEFINTION] and Open Access [REFEREENCE DEFINTION] among others [REFERENCES]. The Turing Way Booklet [@turingwaycommunity2019TuringWayHandbook] provides in-depth definitions and resources on these topics. However, let’s summarize them here to ensure we’re all on the same page. Open Data refers to the openness and accessibility of data. It involves thinking about data sharing, privacy, and protection, as well as considerations like consent and the nuances of handling sensitive data. In the context of computational projects like this PhD, Open Source applies to both software (e.g., programs and applications used) and hardware (e.g., types of machines involved) that are publicly accessible, allowing anyone to view, modify, use, and distribute them, usually under open licensing* [LINK REFERENCE]. Open Access refers to how freely available research content is. There are different ways to achieve this. Some times, open access might involve paying an Article Processing Charge to a journal, which then publishes the final version of the article under an open license, making it permanently free to access online (this is known as Gold Open Access). Sometimes, it may involve self-archiving a version of the research, often alongside preprints, allowing public access without direct journal fees (this is known as Green Open Access).

In order to achieve Open Science, [@turingwaycommunity2019TuringWayHandbook] and [@heise2020OpenAccessOpen] suggest that the research process should:

1.	**Be publicly available**: It's hard to benefit from knowledge hidden behind barriers like passwords and paywalls.

2.	**Be reusable**: Research outputs should be licensed adequately, informing potential users of any restrictions on reuse.

3.	**Be transparent**: With appropriate metadata to provide clear statements of how research output was produced and what it contains.

Additionally, Open Science and its various elements fall within the broader concept of Open Scholarship. Open Scholarship promotes transparency and accessibility in teaching, learning, research, and academia [@emeryLibGuides]. More importantly, Open Scholarship emphasizes equity, diversity, and inclusion, ensuring that knowledge is openly available to everyone, regardless of ethnicity, gender, sexual orientation, or other factors.

It is worth noting that Open Science is not “sharing absolutely everything”. Many fields of science involve working with sensitive personal data, with medical research being the most obvious example, where data is not to be widely shared. Likewise, privacy and data protection, as well as consent, and national and commercially sensitive data can be some of the most common examples of when data cannot always be open [@regulation2016RegulationEU2016]. If access to data needs to be restricted due to security reasons, however, the justification for this should be made clear. Free access to and subsequent use of data is of significant value to society and the economy. The concept of Open Science views that data should, therefore, be open by default and only as closed as necessary [REFERENCE].


## FAIR principles for open and reproducible science
Weaved in with the topics already discussed, are the FAIR principles for scientific data management. These principles were created as a guideline to develop and collectively support a clear and measurable set of principles that allow for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability of digital assets, to ultimately support more reproducible research [@wilkinson2016FAIRa]. FAIR principles, therefore, serve as a valuable framework for conducting research with integrity. To understand the benefit of FAIR principles, let's look at them one by one. Findable data is well-described with rich metadata, making it easier to locate by both humans and machines. Accessible data is stored in a way that ensures users can retrieve it with clear licensing and open protocols. Interoperable data is formatted using standardised vocabularies and structures, enabling seamless integration across different applications or workflows. Reusable data is well-documented, with detailed provenance and clear usage rights, supporting reproducibility and future research.

The advantages of applying FAIR principles become apparent when we understand each principle. For example, when data is more easy to find, more accessible and readable, discoverability naturally improves [REFERENCE]. Beyond this, and more specifically linked to computational projects such as this PhD, FAIR practices can lead to more efficient code, minimising time spent on things like model retraining/rewriting, and reducing redundant data generation and storage. Likewise, the interoperability aspect of FAIR principles enables researchers to integrate datasets from various sources, fostering new insights and innovative problem-solving approaches. Ultimately application of FAIR principles can result in lowering carbon footprint of computer simulations as less time and resources are wasted trying to run models that lacks useful information on how to run it [@lannelongue2023GREENER].

In order to make research FAIR, there are numerous resources out there [REFERENCES] that offer guidance on how to do so. Generally, it is recommended to think about this at the different stages of the research life cycle, as an iterative process. In the [REFERENCE CHAPTER HOW I MAKE THIS PHD REPRODUCIBLE] I will dive deeper as to how these principles are thought about in this PhD and how they have been applied throughout the different stages of the research carried out here.  

## Defining Ethical Science
Open, reproducible and FAIR ideas are fundamentally linked to ethics, as these practices ultimately promote more ethical and responsible research by fostering transparency, integrity, and accountability. Likewise, regardless of how efficient or reproducible a research outcome is, its value is questionable if it causes harm to a group of individuals. Likewise, if a project acknowledges potential biases and risks in its data but fails to provide sufficient resources for others to replicate the research, can we truly call it progress?

This section does not aim to address the extensive literature on ethics [REFERENCES], but rather to provide a clear definition, ensuring a shared understanding on what I mean by "ethical science" moving forward. Scientific ethics refers to the principles and guidelines that govern the behavior of scientists in their work, and their impact in wider society. More broadly, ethics can be seen as a system of moral values and principles that a particular society or community upholds. Ethics can be understood as a core aspect of moral philosophy -an area of study that, among other things, seeks to differentiate between right and wrong. Viewing ethics as the study of what constitutes appropriate or inappropriate behavior emphasizes its connection to action. Therefore, we can see how ethics is relevant when considering the impact of actions and behaviours of scientists throughout the research process. Recognizing this also highlights the fundamental role of moral philosophy and ethics in shaping how research impacts society.

Ethical standards and beliefs evolve over time, and as a result, so do the ethical guidelines that govern scientific research. Practices that were once considered acceptable may no longer align with contemporary ethical norms, just as certain current methodologies may be deemed unethical in the future. Either way, throughout history, including current times, there are numerous examples of scientific research leading to significant ethical concerns and harmful consequences [REFERENCES -see also DATA HAZARDS PAPER FOR FURTHER DISCUSSION]. I believe it is important to recognise the heavy weight that science carries, of not only unethical but also oppressive research that has undermined and hurt minoritised groups and individuals throughout time. Unless we recognise where scientific research stands and where it comes from, we cannot stop repeating the same oppressive behaviours that may go unspoken or unrecognised otherwise. 

In a presentation I delivered in 2022, I provided several examples of historical social biases that persist in scientific research, with a focus on Medicine, Neuroscience and Computer Science, including examples of racism, sexism, ableism, and speciesism (Garcia, Sterratt, and Stefan 2022). I define social bias as a systematic and often unconscious prejudice or favoritism toward certain groups over others, based on characteristics such as race, gender, species, or other socially constructed categories. A good example of embedded biases in science is given by Branch et al. (2022) as they eloquently articulate how a desire to quantify and establish hierarchies among organisms was not purely for scientific interest, but that there is extensive evidence in the fact that the roots of evolutionary biology, which serves as a baseline for many other disciplines like neuroscience, are steeped in histories of white-supremacism, eugenics, and scientific racism. They discuss the definition of the “Not-So-Fit”, and how this limits the diverse thought and investigative potential in biology. This is important to recognise for this thesis, as I use hierarchies and models of biology that are based on a historical context of how science has reached it’s current status of knowledge.

## Why is this important?

Thinking about the (un)ethical history and future implications of the research we do can help to uphold accountability and potentially reduce harm [REFERENCES], as we further discuss in DATA HAZARDS CHAPTER. It is the synergy of combining opennes, reproducibility and ethics that create research that shows integrity and enhances credibility. Thinking about reproducibility can in turn help to think how you will share your data, as well as where your own data has come from. Hence, reaching an increased awareness of how your data was sourced and its ethics and potential biases. Working with ethics, philosophy, reproducibility and an openness to discuss the wider context of where our research rests, may add time to the research timeline, but can very much enrich a fuller and more complex understanding of the shortcomings of our research and how to do better moving forward. This is why I believe that continuous reflection on ethical standards is essential, and why I have made a big emphasis on reiterative reflection throughout this PhD. 

As we have seen, creating research that takes into account opennes, reproducibility andnd ethical considerations offers numerous benefits, not only for researchers but for society as a whole. Researchers can directly benefit through increased visibility and citation rates, greater opportunities for collaboration, and reduced waste of time and resources. Additionally, fostering open scientific practices helps build a stronger sense of community, in contrast to the traditionally competitive nature of research. Embedding these considerations into research workflows can enhance the credibility and reliability of scientific findings. Furthermore, reflecting on the broader impact of research promotes accountability and encourages proactive measures to mitigate potential harm. 

Beyond academic advantages, open, ethical, and reproducible research contributes to more equitable knowledge dissemination. When data, methods, and findings are openly shared, barriers to access are reduced, enabling a wider range of researchers, institutions, and communities to engage with and build upon existing work. This inclusivity can drive innovation, improve public trust in science, and ensure that research benefits can be reaped by as many individuals as possible. 

## How are these principles applied throughout this PhD?
The implementation of open and reproducible practices in this PhD has evolved through multiple iterations. Rather than creating something entirely new, I have drawn on the wealth of existing resources that provide guidance on applying these principles to research. In this section, I aim to illustrate my approach. I reflect on how I have engaged with these principles, adopted the most relevant practices for this project, and integrated them into my research. The result is a PhD that not only considers practical applications of reproducibility but also embeds ethical and philosophical reflections about this works' future impact. To illustrate how I have achieved this, I will walk you through the various projects, posters, and presentations I have developed, highlighting how they have influenced and shaped the work of this PhD.

### Getting started: Bias and reproducibility in a computational neurobiology PhD's journey
A common experience in my academic career has been the limited focus on the social dimensions of ethics in the research I conduct. Ethics was often viewed as something solely for the ethics committee or simply a form to complete, rather than an integral part of the research process. Over time, I developed a keen interest in examining the biases within my own work. This led me to the realization that in order to address and make these biases transparent, my research needed to be more reproducible and accessible. The initial bias I encountered was the assumption that "there is no need to account for social bias in my research", because one might my research topic neutral at first glance: computer simulations of proteins does not scream a need for ethical considerations, mainly because we are not directly using animals, human or non-human. 

To discuss and challenge the idea that my project - as well as many others- does not require ethical considerations, I began exploring how others were addressing bias and reproducibility and created a detailed (albeit wordy) poster, which depicted my PhD as a journey divided into milestones. t each stage, there are both biases that could influence the work and tools to either mitigate or highlight them. Simultaneously, I recognised that for my PhD to be truly ethical, it needed to prioritise reproducibility and accessibility. So, I divided some of the questions I had been asking myself into different sections of the research journey. This is all documented in a GitHub repo that is publicly available [REFERENCE: https://github.com/Susana465/Bias-and-Reproducibility-Poster].  

[![Poster about bias and reproducibility, showing research cycle as a journey which starts with design, then data collection, data analysis and final reporting, and compares this through images to growing an apple tree, collecting the apples and then selling them.](22-reproducibility-figures\20221006_poster_phd_journey.jpg){#fig-posterPhDjourney fig-pos="h" width=500}](https://github.com/Susana465/Bias-and-Reproducibility-Poster/blob/main/20221006_poster_phd_journey.jpg)


This is what they are, why are they important - and this is how I do it (in order of: reproducibility, open science, FAIR principles and ethics - ethics delve into what we will be talking about and the part that is important to this thesis but not full discussion as this is not the place. why is it important. relevance to this phd show strength of it.). Link in with chapter on how I applied all of this into this thesis and PhD project.  in this follow up chapter I can talk about my projects at the ATI and OLS. 




