---
bibliography: [references.bib]
---
# Open, reproducible and ethical science
Intro paragraph

## Reproducibility Crisis

Over the past few decades, scientific research has become increasingly fast-paced, with a "publish or perish" culture firmly embedding itself in the scientific community’s consciousness [REFERENCES]. This mindset exerts significant pressure on researchers to continuously publish in order to secure funding and advance their careers [REFERENCES]. This pressure to publish (predominantly statistically significant results), is closely tied to what is known as publication bias. In published academic research, publication bias occurs when the outcome of an experiment or research study biases the decision to publish or otherwise distribute it. This typically results in a preference for publishing significant or positive results, disrupting the balance of findings and favouring those that are deemed novel or noteworthy. Whilst negative or inconclusive results are rarely published or considered less worthy of publication [REFERENCES].

Examining this bias towards positive results reveals substantial evidence of a troubling trend: an overrepresentation of false-positive findings in the scientific literature. This bias towards positive results has several concerning and wasteful consequences, including, but not restricted to: a significant number of valid negative results remaining unpublished, which excludes critical findings from the scientific record. This means that other research teams, unaware of these unpublished negative results, may continue to unknowingly test the same hypotheses (which may actually be false) until, by chance or artifact, a positive result is obtained. These chance positive results are then published (as they align with the preference for significant findings), even when substantial and more definitive contradictory evidence may exist in contrast to it [REFERENCE]. What is more, this underreporting of negative results introduces bias into meta-analysis, which consequently misinforms researchers, doctors, policymakers and the public in general. Additionally, more resources are wasted on already disputed research that remains unpublished and therefore unavailable to the scientific community. 

This cycle sustains an error-prone body of scientific literature, undermining the reliability of published research. Hand in hand with this publication bias, comes something now known as a _Reproducibility Crisis_, where it is becoming increasingly apparent that making "fast-science" is associated with a lack of being able to reproduce research [REFERENCE]. While one may not directly cause the other, they are closely intertwined. The reproducibility crisis has been in the rise for the past few decades as we uncover the fact that much of the research that is published fails to be reproduced by others. To give some examples, a survey of 1576 scientists published in Nature (Baker, 2016) reported that over 70% of the participants failed to reproduce others’ experiments and over 50% failed to reproduce their own results. Similarly, @tiwari2021ReproducibilitySystemsBiology assessed the reproducibility of 455 mathematical models in systems biology and found that about 50% of published models were not reproducible either due to incorrect or missing information in the manuscript [REFERENCE].

Much of the criticism surrounding the reproducibility crisis centres on statistical methods and research practices [REFERENCES]. Problematic scientific behaviours, such as HARKing (Hypothesising After the Results are Known), p-hacking (manipulating data analysis to achieve statistical significance), and selectively reporting only positive outcomes, as discussed above, have been identified as major contributors to irreproducibility [REFERENCES]. Indeed, this reliance on p-values, provided they meet the conventional threshold of statistical significance (typically p < 0.05), can sometimes lead to studies being treated as definitive evidence, even when their findings are not robust or reproducible [9–11]. 

The list of factors contributing to irreproducibility is extensive, and is not restricted to statistical methods. A lack of access to raw data or, in some cases, outright data fabrication also plays a role [REFERENCE]. Ambiguities in experimental procedures, materials, and data processing steps further undermine research reliability [REFERENCES]. On a broader, more systemic level, there's an important discussion regarding how current academic systems often prioritize novelty and statistically significant findings, as research funding is more likely to be secured and promoted when the outcomes are more profitable [REFERENCES]. Although these behaviours fall under scientific misconduct [REFERENCE: https://publications.parliament.uk/pa/cm5803/cmselect/cmsctech/101/report.html] and are not considered acceptable scientific practice, they continue to occur.

Interestingly, a large-scale survey of nearly 6,000 academic psychologists (with 2,155 responses) assessed self-reported engagement in some of these questionable research practices known to introduce bias into research findings. Notably, respondents often justified their own use of these practices while simultaneously viewing them as unacceptable when used by others. As the authors put it, “Respondents considered these behaviors to be defensible when they engaged in them [...] but considered them indefensible overall” (p. 530). This is just one example, and it is worth noting that scientific standards and methodologies continuously evolve, meaning that questionable research practices are not static. Methods once tolerated, or even considered standard, may now be recognised as problematic, reflecting our ability to reassess and refine ethical and methodological frameworks. Encouragingly, awareness of the reproducibility crisis is growing, and there is a gradual shift towards promoting open and reproducible research practices [@umbach2024Open, @nature2018Challenges]. Before we explore the work being done to promote science as more open, reproducible, and ethical, it’s important to first agree on what we mean by these terms in this thesis.

## Defining Reproducibility

There is a long history of the terms reproducibility and replicability being used interchangeably, or their meanings being swapped depending on the field of study [@claerbout1992ElectronicDocumentsGive], [@ivie2018ReproducibilityScientificComputing],[@plesser2018ReproducibilityVsReplicability]. For example, a review on the usage of reproducible/replicable meanings [@barba2018TerminologiesReproducibleResearch] showed that most papers and disciplines use the terminology as defined by Claerbout and Karrenbach, whereas microbiology, immunology and computer science tend to follow the Associtation for Computing Machinery use of reproducibility and replication given by [@ivie2018ReproducibilityScientificComputing]. In political science and economics literature, both terms are used interchangeably. So this quickly shows how having a lack of agreement on such definitions can add even more confusion to the mix. 

In this PhD, we use the definition used by [@turingwaycommunity2019TuringWayHandbook], where reproducible research is understood as "_work that can be independently recreated from the same data and the same code that the original team used_". Reproducible, replicable, robust and generalisable have different meanings as described in the table below.

![How the Turing Way defines reproducible research.](22-reproducibility-figures/repro_definintions.PNG){#fig-repro-definitions width=65%}

- **Reproducible research**: is obtained when same analysis is performed on the same data, to produce the same results.

- **Replicable research**: refers to conducting the same analysis on different datasets, resulting in qualitatively similar outcomes. 

- **Robust research**: entails subjecting the same dataset to different analysis workflows to address the same research question, such as employing distinct pipelines in R and Python. Robustness demonstrates that findings can remain consistent regardless of different methods used for analysis, indicating validity and resilience to various factors like changes in conditions or methods (such as different programming languages).

- **Generalisable research**: refers to findings or conclusions that can be applied beyond the specific context in which they were derived. It indicates that the results are not limited to a particular dataset, methodology, or experimental setup, but instead can be extended to broader populations, situations, or conditions. By combining replicable and robust research, we can obtain more generalisable results. Generalisability is important as it allows us to make inferences for bigger groups of datasets, for example a target population, by only studying a part of it (the sample).

## Defining Open Science

Open science is an approach to the scientific process that promotes cooperative work and new ways of diffusing knowledge accessible to everyone, without barriers such as paywalls or restrictions on use. By making research more accessible and transparent, open science seeks to accelerate scientific progress, enhance reproducibility, and increase the societal impact of research findings. An whilst there are varying ways of defining what this "opennes" means for different contexts [REFERENCES], a definition provided by @vicente-saez2018Open gives an overall good idea of what open science is referring to: "_Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks [it] helps the scientific community, the business world, political actors, and citizens [...] and stimulates an open debate about the social, economic, and human added value of this phenomenon._" Additionally, The United Nations Educational, Scientific and Cultural Organization (UNESCO) promote the following message in their Recommendation on Open Science: "_By promoting science that is more accessible, inclusive and transparent, open science furthers the right of everyone to share in scientific advancement and its benefits as stated in Article 27.1 of the Universal Declaration of Human Rights_".

Open Science overlaps with other concepts such as Open Data [REFEREENCE DEFINTION], Open Source Software and/or Hardware [REFEREENCE DEFINTION] and Open Access [REFEREENCE DEFINTION] among others [REFERENCES]. The Turing Way Booklet [@turingwaycommunity2019TuringWayHandbook] provides in-depth definitions and resources on these topics. However, let’s summarize them here to ensure we’re all on the same page. Open Data refers to the openness and accessibility of data. It involves thinking about data sharing, privacy, and protection, as well as considerations like consent and the nuances of handling sensitive data. In the context of computational projects like this PhD, Open Source applies to both software (e.g., programs and applications used) and hardware (e.g., types of machines involved) that are publicly accessible, allowing anyone to view, modify, use, and distribute them, usually under open licensing* [LINK REFERENCE]. Open Access refers to how freely available research content is. There are different ways to achieve this. Some times, open access might involve paying an Article Processing Charge to a journal, which then publishes the final version of the article under an open license, making it permanently free to access online (this is known as Gold Open Access). Sometimes, it may involve self-archiving a version of the research, often alongside preprints, allowing public access without direct journal fees (this is known as Green Open Access).

In order to achieve Open Science, [@turingwaycommunity2019TuringWayHandbook] and [@heise2020OpenAccessOpen] suggest that the research process should:

1.	**Be publicly available**: It's hard to benefit from knowledge hidden behind barriers like passwords and paywalls.

2.	**Be reusable**: Research outputs should be licensed adequately, informing potential users of any restrictions on reuse.

3.	**Be transparent**: With appropriate metadata to provide clear statements of how research output was produced and what it contains.

Additionally, Open Science and its various elements fall within the broader concept of Open Scholarship. Open Scholarship promotes transparency and accessibility in teaching, learning, research, and academia [@emeryLibGuides]. More importantly, Open Scholarship emphasizes equity, diversity, and inclusion, ensuring that knowledge is openly available to everyone, regardless of ethnicity, gender, sexual orientation, or other factors.

It is worth noting that Open Science is not “sharing absolutely everything”. Many fields of science involve working with sensitive personal data, with medical research being the most obvious example, where data is not to be widely shared. Likewise, privacy and data protection, as well as consent, and national and commercially sensitive data can be some of the most common examples of when data cannot always be open [@regulation2016RegulationEU2016]. If access to data needs to be restricted due to security reasons, however, the justification for this should be made clear. Free access to and subsequent use of data is of significant value to society and the economy. The concept of Open Science views that data should, therefore, be open by default and only as closed as necessary [REFERENCE]. 

## Defining Ethical and FAIR Science
Open and reproducible science is fundamentally linked to ethics, as these practices promote more ethical and responsible research by fostering transparency, integrity, and accountability. In the context of research, scientific ethics refers to the principles and guidelines that govern the behavior of scientists in their work, and their impact in wider society. More broadly, ethics can be seen as a system of moral values and principles that a particular society or community upholds. 

This section does not aim to address the extensive literature on ethics, but rather to provide a clear definition, ensuring a shared understanding on what I mean by "ethics" moving forward. In this way, ethics can be understood as a core aspect of moral philosophy -an area of study that, among other things, seeks to differentiate between right and wrong. Viewing ethics as the study of what constitutes appropriate or inappropriate behavior emphasizes its connection to action. Therefore, we can see how ethics is relevant when considering the impact of actions and behaviours of scientists throughout the research process. Recognizing this also highlights the fundamental role of moral philosophy and ethics in shaping how research impacts society.

Ethical standards and beliefs evolve over time, and as a result, so do the ethical guidelines that govern scientific research. Practices that were once considered acceptable may no longer align with contemporary ethical norms, just as certain current methodologies may be deemed unethical in the future. Either way, throughout history, including current times, there are numerous examples of scientific research leading to significant ethical concerns and harmful consequences [REFERENCES -see also DATA HAZARDS PAPER FOR FURTHER DISCUSSION]. I believe it is important to recognise this heavy weight that science carries, of not only unethical but also oppressive research that has undermined and hurt minoritised groups and individuals throughout time. Unless we recognise where scientific research stands and where it comes from, we cannot stop repeating the same oppressive behaviours that may go unspoken or unrecognised otherwise. 

In a presentation I delivered in 2022, I provided several examples of historical social biases that persist in scientific research, including examples of racism, sexism, ableism, and speciesism (Garcia, Sterratt, and Stefan 2022). I define social bias as a systematic and often unconscious prejudice or favoritism toward certain groups over others, based on characteristics such as race, gender, species, or other socially constructed categories. A good example of embedded biases in science is given by Branch et al. (2022) as they eloquently articulate how a desire to quantify and establish hierarchies among organisms was not purely for scientific interest, but that there is extensive evidence in the fact that the roots of evolutionary biology, which serves as a baseline for many other disciplines like neuroscience, are steeped in histories of white-supremacism, eugenics, and scientific racism. They discuss the definition of the “Not-So-Fit”, and how this limits the diverse thought and investigative potential in biology. This is important to recognise for this thesis, as I use hierarchies and models of biology that are based on a historical context of how science has reached it’s current status of knowledge.

Thinking about the ethical history and future implications of the research we do can help to uphold accountability and potentially reduce harm [REFERENCES], as we further discuss in DATA HAZARDS CHAPTER. This is why I believe that continuous reflection on ethical standards is essential, and why I have made a big emphasis on reiterative reflection throughout this PhD. Additionally, it is the synergy of combining opennes, reproducibility and ethics that create research that shows integrity and enhances credibility.

Weaved in with the topics already discussed, are the FAIR principles for scientific data management. These principles were developed as a guideline to "design and jointly endorse a concise and measureable set of principles" that allow for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability of digital assets, to ultimately support more reproducible research [@wilkinson2016FAIRa].




In CHAPTER [DATA HAZARDS PAPER]. 

And so as ethical values vary and evolve throughout time, ethical guidelines also vary between who is defining them; different bodies define ethical research using different definitions. For example, the National Institutes of Health (NIH) defines research as ethical if it follows the guidelines of being of social and clinical value, that it upholds scientific validity, that has fair subject selection, that has a favorable risk-benefit ratio, follows independent review and informed consent, and finally research that shows respect for potential and enrolled subjects. Likewise, ethical considerations vary depending on the topic of research, as different factors need to be taken into account. Research that deals directly with human patients will need different ethical considerations to research that studies protein dynamics using computer simulations.

Here we focus on research ethics that is most applicable to Neuroscience and data intensive projects generally, but will discuss in depth ethical considerations to be taken for this PhD project in particular too. 

from ttw:
_With no intention of doing justice to the vast literature that there is on the topic, we can take Ethics to be the subject of study of moral philosophy, which seeks – amongst other things – to distinguish right from wrong. Nowadays, philosophers speak of three levels of discussion in ethics [Dit22]: metaethics, normative ethics and applied ethics. Roughly, metaethics questions the nature of good and evil, as well as the underlying assumptions held by different ethical frameworks. Normative ethics, then, provides the tools for us to create such ethical frameworks and think within them. Applied ethics, finally, is what is most pertinent to this guide, as we will be discussing the ins and outs of ethical and responsible research and innovation._

_Taking ethics as the study of what behaviours are right and what behaviours are wrong highlights that it refers to action. Considering this, ethics is relevant to research insofar that researchers are acting and behaving in certain ways when conducting their work. Taking this definition seriously also means accepting moral philosophy as paramount in our everyday lives: ethics pertains to us all. Furthermore, the potential ripple effects of any research project may be far greater than an individual’s actions under normal circumstances._

_To this effect, the importance of acting ethically within the academic sphere is heightened. Scientists must reflect on the ethical questions their research projects raise._

_One source of diversity of ethical frameworks relates with how research institutions generally have “Ethics Committees” that appraise the ethics of research conducted within their institutions. This is traditionally done for institutions to protect themselves from legal liabilities. However, further reflection – beyond questions of data protection – can be required by these processes. The point is that one university’s process for ethical appraisal will, more often than not, differ from another’s._

Two common key concepts that consider Ethics in research are "Responsible Research and Innovation" (RRI) and "Research Integrity (RI)". The former revolves around an outward impact of research, for example, "how may individuals be impacted by this research?" or "what is the view of the public on this kind of research"?. Research Integrity revolves around how research is conducted, for example asking questions such as "how are scientists conducting themselves in this research?". 

We might like to think that scientific research is objective, and simply measuring reality. The reality, however, is that many decisions will be taken when doing research, and these decisions will impact the results of that research. Decisions taken in any kind of research can and will vary depending on the background of the human making these decisions, their socio-economic upbringing, religious beliefs and personal biases will affect the research even if it is unconsciously. 

Science and philosophy both seek to understand the world, but they take different approaches nowadays. Science relies on empirical evidence and aims for practical applications, while philosophy explores conceptual frameworks and seeks a deeper understanding of the fundamental nature of reality. While these distinctions help clarify the differences between Science and Philosophy, it's essential to note the overlap and interdisciplinary collaboration between the two fields. Some questions may be addressed by both science and philosophy, and insights from one field can inform the other. This separation often makes it seem like you have a choice to make: are you going to do Philosophy, or are you going to do Science? 

When I studied Neuroscience as an undergraduate, I remember wanting to discuss more of the Philosophy and Ethics of the subjects in my degree: Why do we study this topic in this way, what are the possible consecuences of this research, how ethical is this research on animals? Instead, the most philosophising I got was learning about the different streams of consciousness theories, and that's about it. Even now, I find a constant separation between Philosophy and Neuroscience - the call for this paper makes it clear: *"Neuroscientists share common interests with philosophers, but the two groups use very different approaches"*. The apparent separation of Philosophy from STEM (Science, Technology, Engineering and Mathematics) and viceversa is an unfortunate development of the 19th century, as this separation has not always existed [REFERENCE]. I call it an "apparent separation" because although it may seem they have been separated as we continue to study them individually, they are persistently intertwined. In my opinion, it is not only impossible to separate them, but I argue that separating (Neuro)Science from Philosophy can be a dangerous thing. In this paper, I'll explore potential dangers across my PhD research life cycle, offer neuroscience examples, and demonstrate how the Data Hazards framework aids in discussing philosophical and ethical issues in neuroscience.

 - probably want to add philosophy and ethics discussion after repro section and LINK WITH DATA HAZARDS. 


## Why are reproducibility and open science important?
Creating open practices has multiple benefits. Firstly, researchers can benefit from it first hand by creating open access articles, as these have been shown to be cited more often [@mckiernan2016HowOpenScience]. Another benefit of openness is that while research collaborations are essential to advancing knowledge, identifying and connecting with appropriate collaborators is not trivial. Open practices can make it easier for researchers to connect by increasing the discoverability and visibility of one’s work, facilitating rapid access to novel data and software resources, and creating new opportunities to interact with and contribute to ongoing communal projects. Creating science that follows the open definitions above can therefore also foster a deeper sense of community, which contrasts starkly with the usual competitiveness of science. 

