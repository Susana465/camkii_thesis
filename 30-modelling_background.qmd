---
bibliography: [references.bib]
---

# Computational Modelling Background{#sec-intro-comp}
Computational modelling offers a powerful means of investigating complex biological systems by simulating molecular interactions. These models aim to describe the elements of a system, their states, and their interactions with sufficient precision to replicate real-world dynamics. Biological systems, such as cells, operate through multiscale processes in which molecular interactions give rise to larger-scale cellular behaviors [@rothschild2006Role; @suki2011Complexity]. For example, in neurons, molecular interactions like neurotransmitter release, receptor activation, and intracellular signalling regulate synaptic plasticity. These interactions give rise to emergent behaviours, where molecular and cellular changes scale up to influence higher-order functions. Changes in synaptic strength, such as Long-Term Potentiation (LTP) and Long-Term Depression (LTD) (see @sec-LTP-to-explain-memory), drive larger-scale processes like dendritic spine remodelling and receptor trafficking, ultimately influencing neural circuit activity —key mechanisms underlying learning and memory.

To study intricate cellular processes such as signal transduction and neuronal plasticity we require appropriate mathematical tools and models capable of simulating dynamic, nonlinear interactions at multiple scales. A computational model incorporates numerous variables representing the system under study, with simulations performed by adjusting these variables —either independently or in combination —and observing the outcomes [@hassan2023Perspectives]. Computational models allow for the exploration of the system's emergent behaviors, where system-level properties arise from simple interactions among individual components but are not explicitly encoded at the molecular level [@xiao2015Emergent].

This PhD employs computational modelling to explore biological systems, focusing on biochemical modelling of dynamic, complex reaction networks that underpin biological processes. Specifically, we look at the interactions between CaMKII and NMDAR in the postsynaptic dendrite of neurons that are thought to be relevant for learning and memory (see @sec-biology-chapter). The rationale for selecting specific modelling approaches to study the interactions between these molecules are introduced and discussed next. These approaches include rule-based modelling, agent-based modelling and stochastic simulation algorithms. This chapter also discusses the computational tools used to implement these models, including Monte Carlo Cell (MCell) [@stiles1996Miniaturea; @2000Monte; @kerr2008FAST; @husar2022MCell4] for stochastic spatial simulations and Biological Network Generator (BioNetGen) [@harris2016BioNetGena] for rule-based modelling. Together, these tools provide a robust framework for simulating complex molecular interactions and cellular behaviours, enabling the study of dynamic processes that can often be difficult to investigate experimentally.

## Why use computational modelling to study biological systems?
Computational models are becoming increasingly popular in Neuroscience (and in the sciences in general) [@noble2002Rise; @kaur2021Computational; @levenstein2023Role], as they provide useful advances to enhance our understanding of systems that want to be studied. When modelling biological systems, we aim to describe the system's elements, states, and interactions to accurately simulate its behaviour. In the context of this PhD study, we examine postsynaptic neuronal dendrites, where complex biochemical processes span multiple compartments and scales.

Given the vast complexity of reactions and molecular networks within a dendrite, mathematical tools used in computational modelling can help simplify the analysis of these systems. This way, models allow questions to be investigated that are difficult to approach experimentally. By focusing on specific parts of biological multiscale dynamics, these tools allow researchers to isolate particular aspects of interest. This approach helps minimize the influence of unrelated variables and reduces the complexity of the full biological system. Computer simulations allow adjustments to almost any state variable at any given time point, and they enable data to be analysed with virtually any level of precision, both in terms of time and space [@levenstein2023Role].

It is important to note that when modelling, just like with wet-lab, care must be taken when interpreting simulation results, as they are subject to finite accuracy. Errors can arise from a variety of reasons, such as the properties used in simulations, the mathematical models, their numerical algorithms, and more (this is further discussed in @sec-data-hazards-chapter). Moreover, biological systems are inherently stochastic and exhibit natural variability [@teschendorff2021Statistical; @demopoulos2025Chaos]. For computational results to be meaningful, they must account for the full range of expected variability in the physical systems they aim to represent. That is why simulations are carefully validated by comparing their behaviour to experimental data; and inconsistencies can suggest incomplete assumptions or misinterpretations. However, it is also important to recognize that even if a model successfully recapitulates experimental results, this does not necessarily mean it is correct. Biological systems can often exist in multiple states that produce the same observable outcome, meaning that different underlying mechanisms could lead to similar results.

Models that survive this initial validation can be used to predict new outcomes and explore scenarios that are challenging to investigate experimentally, offering insights into the workings and principles of biological systems. This way, modelling aims to clarify the current state of knowledge about a system by precisely defining its elements and interactions, which can reveal gaps in understanding [@szekely2014Stochastic]. Whilst it is true that models may omit certain details of a biological system, the purpose of computational models is not to replicate every detail of a biological system, but rather to offer a specific representation that captures the essential dynamics relevant to a specific research question. By focusing on key components and interactions, models offer the capability of testing minimal requirements of very complex systems by just considering the molecules immediately influencing the phenomenon that is being studied.

Furthermore, models can be refined iteratively as new data becomes available or as understanding of the system deepens, facilitating continuous improvement and development of the system at hand. Even if a model does not encompass every element of the system, it remains a powerful tool for identifying critical relationships, predict system behaviour, guiding experimental design, generating hypotheses and offering structured frameworks for exploring complex biological phenomena [@szekely2014Stochastic; @brodland2015How].

![Adapted from @szekely2014Stochastic](30-modelling-figures\dry_wet_lab_cycle.PNG){#fig-dry_wet_lab_cycle.PNG}

Moreover, using computational methods for studying biological mechanisms can be time and cost-efficient. Modelling is well positioned for integration into the experimental cycle of biology. Although _in vitro_ and _in vivo_ experiments are still needed to advance our understanding of biological processes, conducting _in silico_, or computer-simulated experiments can help guide the wet-lab process by narrowing the experimental search space [@robinson2004Simulation]. Importantly, this creates an iterative cycle, where computational models inform experimental design, and experimental findings, in turn, refine and improve the models. This continuous process strengthens both approaches, leading to a deeper and more accurate understanding of biological systems.

In addition to the reasons discussed, my preference for computational modelling over wet-lab experiments is also shaped by personal considerations. Among these personal reasons, is my desire to move away from animal experimentation, as the speciesist assumption that testing on animals is both acceptable and necessary does not align with my values. A detailed and nuanced discussion of this perspective is provided in [@sec-data-hazards-chapter]. Following the principles of the “3Rs alternatives” framework —replacement, reduction, and refinement of animal use in research— I am particularly committed to the goals of replacement and reduction.

In my opinion, computational modelling research such as the one undertaken in this PhD, cannot be fully separated from animal research because experimental data is often derived from studies that have involved using animals to a certain degree. Moreover, models must be validated against experimental results to confirm their biological relevance, and in many cases, such validation still depends on data from _in vivo_ or _in vitro_ studies. Nevertheless, I aim to advocate for a shift toward computational research that not only minimizes reliance on animal studies but also emphasizes reproducibility. Ensuring that research is reproducible helps avoid redundant experimentation by allowing scientists to verify and build upon existing findings rather than repeating studies due to unclear methodologies or irreproducible results. When computational models are rigorously validated and openly shared, they can serve as reliable references, reducing the need for additional animal-based experiments to confirm previous findings. This is also further discussed in @sec-open-repro-chapter.

## How do we model biochemical systems networks?
When constructing a computational model for biochemical system networks, one of the first steps researchers must undertake is identifying the key components of the system and the critical interactions between them. There is no single applicable method for this, as the approaches and questions will vary depending on the specific study being conducted, just like in wet-lab experiments. For instance, is it necessary to represent the behaviour of the system in three dimensions, or would a two-dimensional approximation be sufficient, or is spatial information necessary at all? Similarly, should individual cells and molecules be explicitly modelled, or can their behaviour be adequately captured using appropriate constitutive equations? Other factors to consider include determining which interactions between components are essential for the model and assessing the role of stochastic influences on these interactions. These examples are not exhaustive, as the questions and decisions depend on the system studied, the hypotheses being tested, the study approach, and more. A detailed description of factors chosen for the model in this project are given in SECTION-METHODS-TBC. Although each assumption made at this stage may introduce differences between the model and the biological system it seeks to emulate, well-considered assumptions -chosen based on biological knowledge, empirical evidence, and modelling best practices- can improve the model's clarity, providing a solid foundation for effective representation of the biological system. 

Deciding which simplifications and choices to make during the creation of a model requires a deep understanding of the system combined with mathematical and computational skills. Modelling biochemical systems networks is usually a combined effort between modellers and biologists (although these roles are not exclusive). This interdisciplinarity is a key strength of the process, as it means modelling combines diverse expertise, encouraging are more holistic understanding of diverse perspectives.

Once the key factors to be modelled have been identified, conceptual elements of the model can be translated into a mathematical framework. This process involves defining aspects such as state variables, as well as representing components of the system using appropriate data types to model the system [REFERENCES]. Each interaction within the system may then be characterised by appropriate mathematical relationships too. For instance, biochemical reactions can be expressed through differential equations that describe the rate of change in the concentrations of reactants and products. The implementation of suitable algorithms to accurately realise the various components of the model is critical in ensuring its functionality and efficiency. 

For example, is deterministic modelling or stochastic modelling more suitable for studying a particular system of interest? See @sec-ode and @sec-ssa. Additionally, there is an expanding array of tools and modelling approaches available that researchers can choose from [@ghosh2011Software; @olivier2016Modeling;@bartocci2016Computational; @kohl2011Standards]; translating biological processes into mathematical models on a computer requires not only the appropriate algorithms but also a suitable programming language. There is a wide range of programming languages to choose from —such as Python, MATLAB, and C to name a few- each with its own advantages and limitations. In the following sections, we will explore the modelling approaches and programming languages that are relevant to this PhD.

### Reaction-based modelling:{.unnumbered}
When translating biological and chemical systems (such as CaMKII and NMDARs interactions in the postsynaptic neuron) into algorithms, one way of describing these systems can be through networks of chemical reactions. This approach is known as _reaction-based modelling_, where mathematical models are used to represent the dynamics of biological systems in a computer [@besozzi2016ReactionBased]. Reaction-based models are formalized as sets of reactions that describe the given system in terms of mechanistic interactions between the species of interest. This is, biochemical networks are a set of chemical species that can be converted into each other through chemical reactions. 

The focus of biochemical network models is usually on the levels of the chemical species and this usually requires explicit mathematical expressions for the velocity at which the reactions proceed (kinetic reaction rates). Once kinetics have been specified, these systems can be used directly to construct full dynamic simulations of the system behaviour.

Biochemical network models allow us to gather insight by simulating chemical interactions over time; we can observe changes in species levels, visualise stable states within the system, and look for potential direct or indirect causal relationships between the species being studied. Importantly, the modellers can modify any of these parameters to test how such changes impact the model’s results. Computational models allow us to modify parameters in ways that would be extremely difficult or even impossible to test experimentally, such as precisely controlling molecular concentrations of molecules in particular states, eliminating specific interactions, or simulating conditions that cannot be easily replicated in a laboratory setting. Using a reaction-based approach is particularly useful for modelling the interactions of molecules of interest in this PhD, where we can simulate various chemical components such as NMDARs and CaMKII among others and their interactions in an efficient manner, as discussed in SECTION-MODELLING-CAMKII.

Biological systems can be simulated in different ways using different algorithms depending on the assumptions made about the underlying kinetics, as we will see below; and different formalisms are usually applied to describe the dynamics of these biochemical systems. The kinetics of chemical reactions vary based on the timing of molecular interactions, with reactions unfolding over a timescale determined by the microscopic mechanics involved. Molecular collisions occur randomly inside cells, and are influenced by factors like thermal motion and diffusion. This randomness means that the number of molecules of a particular species fluctuates as a random variable. 

However, when we observe large-scale, or macroscopic, quantities —such as the concentration of a substance over time— the outcomes tend to be consistent and predictable. This predictable trend enables us to develop rate laws, mathematical expressions that describe how the concentration of molecules changes over time. Rate laws are foundational to deterministic modelling, as they assume that, given a specific starting point (initial conditions), the progression of a chemical process is fixed or "predestined." Deterministic models thus allow scientists to predict the time evolution of chemical concentrations with high accuracy, even if the underlying molecular interactions remain random on a microscopic scale. Deterministic models work well where molecular species exists in vast quantities. However, as systems decrease in scale -such as in the confined environment of a cell's cytosol- random fluctuations in molecular populations become significant, making experimental results less reproducible and measurements more variable. Unlike deterministic models, which assume smooth, predictable changes, stochastic models accommodate the random fluctuations in molecule numbers that can significantly impact reaction outcomes in confined environments. Lets briefly examine the reasons why each of these approaches may be employed for distinct purposes:

### Deterministic modelling:{.unnumbered}{#sec-ode}
Deterministic approaches to chemical kinetics are often used to characterize time evolutions of chemical reactions in large systems. A popular representation for these models is to use ordinary differential equations (ODEs) to describe the change in the concentrations of chemical species. Running the same set of parameters using deterministic simulations will produce the same results each time by solving these ODEs. Such descriptions are appropriate when the number of particles involved in the biochemical network is large enough to be able to consider continuous concentrations and when spatial effects are negligible, i.e. well-mixed environment is assumed and space has no effect on reactions. In ODE-based models, each chemical species in the network is represented by an ODE that describes the rate of change of that species along time. Therefore, ODE models of biochemical processes are useful and accurate in the high-concentration limit, but often fail to capture stochastic cellular dynamics accurately because the deterministic continuous formulation assumes spatial homogeneity and continuous molecular concentrations.

These ODE models can be used to simulate the dynamics of the concentrations of the chemical species along time given their initial values. This is achieved by numerical integration of the system of ODE which can be carried out with well-established algorithms [REFERENCES]. They are also useful to find, for example, steady states of the system, which are conditions when the concentrations of the chemical species do not change [@maly2009Introduction].

### Stochastic modelling:{.unnumbered}{#sec-ssa}
As a general rule, stochastic simulations are preferred where the numbers of particles of a chemical species is small; the ODE approach is required when the number of particles is large because the stochastic approach might be computationally intractable. However, when the assumption of continuous concentration fails due to small-scale cellular environment with limited reactant populations ODE representation also fails. It is here when stochastic simulations are useful. Unlike ODE models, which assume continuous concentrations, stochastic simulations track individual reaction events using probability distribution functions. Stochastic Simulation Algorithms (SSAs) [REFERENCES], determine the timing and sequence of reactions based on these probabilities.

It is important to stress that one simulation run according to stochastic approaches is only one realization of a probabilistic representation, and thus provides limited amount of information on its own. When running stochastic simulations, it is important that they are repeated for a sufficient number of times in order to reveal the entire range of behaviour presented by such a system i.e., to estimate a probability distribution for each chemical species and its dynamic evolution. The number of required runs depends on the system's complexity and the degree of variability in its dynamics, with guidelines available in the literature [REFERENCES] for determining sufficient sampling (for example based on statistical confidence intervals).

Ultimately, both SSAs and ODEs offer valuable modelling insights, and the selection of the appropriate method depends on the specific dynamics and the level of detail required for simulating the biochemical network under investigation.

Finally, and all throughout, model validation is a crucial step in ensuring that the system, once translated into a computer model, aligns with biologically relevant parameters and operates within established ranges. Like the model-development process itself, validation is typically iterative, with the model being developed incrementally and each step building on solid foundations established through prior modelling and validation. A key aspect of this process is the creation of modular structures, where a complex system is broken down into smaller, independent, and self-contained units, or "modules." Each module represents a distinct part of the system with a specific function, and these modules can be developed, tested, and maintained separately. This modular approach promotes reusability, scalability, and flexibility because modules can be easily updated or replaced without affecting the entire system. In the METHODS-SECTION we will explore in detail how the model in this PhD was built with reproducibility and robustness in mind. 

As we have seen, computational modelling has its benefits for studying biological systems. However, the complexity of these systems presents unique challenges. Biological systems consist of numerous interacting components and emergent behaviours, requiring careful consideration of both individual components and their dynamic interactions. The next section delves deeper into the concept of complexity in biological systems, examining the challenges of modelling such networks, and how these challenges are directly linked to the biological questions of this PhD. I will also discuss the modelling approaches I employ to handle the complexity of these systems.

## Complexity in Systems Biology: Modelling Approaches and Challenges {#sec-compexity-systems-biology}
This PhD thesis is grounded in _Complexity Science_, which studies how interactions between components of a system give rise to emergent behaviors. Examples of application of Complexity Science can be predator-prey models, epidemiological modelling of pandemics, protein-protein interaction networks, models of neurons, and more. When applied to biology, complexity science often falls under the banner of _Systems Biology_. Systems biology refers to the quantitative analysis of dynamic interactions between multiple components within a biological system, with the goal of understanding the system's behaviour as a whole. Systems biology entails the study of complex biological systems by combining mathematical modelling, computational simulation, and biological experimentation. 

Complexity is a critical aspect of the systems studied in this project, that is, the spatiotemporal dynamics of molecules such as CaMKII and NMDARs in the PSD; making it essential to understand our choices in how and why we model these elements. Specifically, we are interested in catalytic interactions that drive post-translational modifications such as enzyme-driven phosphorylation of CaMKII and NMDARs, as well as interactions between these proteins that promote the assembly of molecular complexes. These types of interactions are hallmark drivers of something known as _combinatorial complexity_ in cellular systems and biochemical networks. 

When proteins interact, they create unique states that alter the protein’s function, structure, or binding capabilities. Meaning that each protein can undergo multiple types of modifications or engage in various site binding interactions, which leads to a multitude of distinct protein configurations. Systems of interacting proteins are inherently complex as the interactions between its constituent proteins usually have the potential to create a vast array of distinct chemical species. This number can far exceed the actual count of proteins or protein interactions within the system itself, creating a combinatorial explosion where every modification or binding event adds another layer of potential molecular arrangements. This high number in potential molecular arrangements is referred to as _combinatorial complexity_, a defining feature of cellular systems and biochemical networks [REFERENCES].

Moreover, these distinct molecular species states do not exist in isolation. They are interconnected through an extensive network of reactions, further amplifying the system's complexity. Intricate network of protein–protein interactions is in fact a prominent feature of any signal-transduction system [@gomperts2009Signal; @hunter2000Signaling]. These interactions can occur at multiple levels, including feedback loops, cross-talk between pathways, and spatial-temporal variations. Each reaction acts as a link within a larger network, creating pathways that connect different species. Modelling these networks is therefore particularly complex due to the difficulty in capturing all possible interactions and the uncertainty in the exact nature of these interactions; i.e. not all possible states of a molecular species may be relevant for its functions.

The magnitude of combinatorial complexity can be exemplified well with the CaMKII holoenzyme studied in this PhD. CaMKII is a multi-subunit protein that can exist in a vast number of possible functional states, depending on the modifications and interactions at each subunit. Each of the individual subunits of CaMKII can have multiple possible states, influenced by factors such as phosphorylation, binding to calmodulin, and interactions with other proteins like NMDARs, and more [@nicoll2023Synapticc]. It has been proposed that a CaMKII dodecamer could potentially exist in as many as $10^{20}$ possible states (see calculations in METHODS AND appendix S1 in @pharris2019Multistate). Moreover, the fact that the potential states of CaMKII vastly outnumber the actual CaMKII molecules in a dendritic spine [REFERENCE], suggests that not all states occur with the same frequency and therefore not all mathematically calculated states are of equal biological relevance, which highlights the challenges of modelling systems that involve combinatorial complexity. 

Combinatorial complexity of signalling systems involving multi-state proteins then impacts both the process of writing the model, known as the _specification problem_, and the computational demands of running it, particularly in terms of time and resources: the _computation problem_ [@stefan2014Multistate]. The specification problem encompasses challenges such as representing all possible states of a molecule like CaMKII, the various complexes it can form, the transformations those complexes can undergo, and the parameters and conditions that govern these processes, all in a robust and efficient manner. When faced with the computation problem, where it may become computationally infeasible to enumerate or simulate all state and network possibilities, the model may become intractable, requiring excessive time and/or resources to compute, store and/or analyse. 

As explored next, rule-based modelling provides a powerful solution to the specification problem: rather than accounting for every possible state explicitly, it enables a focus on biologically significant states while allowing emergent behaviours to arise through rule-based interactions. As for the computation problem, we will see later on how particle-based rule evaluations can help with it. 

## Rule based modelling 
The challenges posed by the specification problem of combinatorial complexity motivate the adoption of a rule based modelling (RBM) approach for simulating cell signalling systems in this PhD. RBM can help by using a set of logical rules to describe how molecules interact with each other. With this type of modelling, the system is modelled by specifying the reactions (or rules) that describe how molecules interact and change. Instead of listing every individual interaction or molecular species explicitly, rules are used to represent how molecules bind, modify, and transform. These rules, which are primarily based on experimental observations, are applied to sets of molecules that can be in different states or configurations, and the overall dynamics of the system emerge from the repeated application of these rules. This method allows for more efficient specification of models, and can scale to handle complex systems like the ones studied in this PhD; this way, avoiding the impossible enumeration of the $10^{20}$ calculated states of CaMKII alone. 

### BioNetGen: A tool for rule based modelling {.unnumbered}
BioNetGen (Biological Network Generator) [@harris2016BioNetGena] is a useful tool for performing RBM of biochemical networks, particularly in the context of signalling systems and protein–protein interactions. BioNetGen uses a language called BNGL (BioNetGen Language) to define molecular species and the rules that govern their interactions. A key feature of BNGL is that it allows researchers to describe the interactions between proteins and other molecules in a high-level, abstract form, without the need to manually enumerate every possible molecular species or state. Instead, BNGL allows users to define generalized rules using an RBM approach, specifying how molecular entities can interact -such as binding, phosphorylation, or degradation — along with the conditions under which these interactions occur.

Different approaches to RBM are used depending on one's objectives and modelling focus [@schaff2016Rulebased; @faeder2009Rulebased; @lopez2013Programming; @boutillier2018Kappa]. In our approach, we employ BNG and its associated language BNGL as it provides some key advantages for the research at hand. Firstly, BNGL provides a language that is tailored towards modelling biochemical networks with domain-specific requirements, allowing for the detailed specification of molecules and their binding domains, which is particularly useful for studying protein post-translational modifications, for example autophosphorylation of CaMKII. Moreover, BNGL helps resolve nomenclature challenges by allowing explicit naming of molecules, binding sites, and modification states in a structured and consistent manner. The ability to model site-specific details of protein-protein interactions allows these dynamics to be captured systematically.

Guided by the "don't care, don't write" principle at the heart of RBM modeling, BNGL allows us to specify only the relevant states for a given reaction. This approach eliminates the need to define every possible state explicitly. That is, the rules determine when an implicitly defined reaction can happen and then, for any given iteration, only the states that matter for the execution of a particular reaction (or rule) are explicitly declared. States that do not matter to a particular rule can be omitted. For instance, lets consider a reaction rule where a CaMKII subunit binds to a CaM molecule (@fig-bngl-rbm-example). 

![A and B molecule binding causes phosphorylation; an example of don't care, don't write in BNGL. The possible states of molecules A and B are defined under the molecule types block. Molecule A has a binding site (b) and a phosphorylation site (T286), which can exist in a phosphorylated (~P) or unphosphorylated (~0) state. Molecule B is defined simply with a single binding site (a). The initial states of these molecules are defined in the species block, which specifies how the molecules are introduced at the start of the simulation. Molecule A is released with its binding site (b) free (unbound to B) and its phosphorylation site in the unphosphorylated state (T286~0). Molecule B is released in a free state, unbound to molecule A. The reaction rules block defines how the molecules interact. In this example, the rule specifies that phosphorylation occurs when molecules A and B bind (indicated by the "!1" link notation). Importantly, the state of A’s phosphorylation site does not need to be explicitly stated in the rule. What is required for the reaction to occur is the binding of A and B, after which phosphorylation can take place. While this is a simple example with limited complexity, as more possible states are introduced, the value of this method becomes clearer. Note this example does not represent a complete BNGL model, as indicated by the "[...]", which denote additional model definitions that would typically appear before and after these lines.](30-modelling-figures\AB_phospho.PNG){#fig-bngl-rbm-example}

In the example shown in @fig-bngl-rbm-example, we don't specify the state of T286 in the reaction rule of A and B leading to phosphorylation of the former. Instead, we only specify the states that are relevant for a specified reaction (in this case, that molecule A has a free binding site to bind with B, and vice versa) and the rest is left unspecified. In more complex models (see REFERENCE MODEL IN THIS PHD) this dramatically reduces the number of reactions that need to be written [@karp1975Computational; @karp1972Reducibility; @blazewicz2005Selected; @suarez2009Challenges]. Details of how this is done in our model are described in LINK-SECTION-TBC. 

Additionally, BNGL supports the inclusion of cellular compartments through its compartmental extension (cBNGL), enabling explicit modelling of the compartmental organization of the cell and its effects on system dynamics. This way, we can introduce localization attributes for molecular species, as well as appropriate volumetric scaling of reaction rates. BioNetGen and its associated language BNGL have been integrated with MCell via pyBNGL, a Python library [REFERENCE]. As we will see next, this provides further advantages when it comes to the computation problem of combinatorial complexity. 

### MCell: A tool for dynamic protein visualisation and modelling{#sec-MCell .unnumbered}
MCell (Monte Carlo Cell) is an agent-based (also known as particle-based) reaction-diffusion software platform designed to simulate complex biochemical processes with a focus on spatial and stochastic dynamics [@stiles1996Miniaturea; @2000Monte; @kerr2008FAST; @husar2022MCell4]. Agent-based modelling simulates the interactions of autonomous agents -such as molecules- to understand how a system behaves and to capture emergent behaviours; for example, the formation of molecular complexes, signal transduction pathways, or spatial patterning within a cell. 

MCell is particularly valuable for modelling biological systems where the precise location and movement of molecules are crucial, such as synaptic signalling or interactions at the cellular level studied in this PhD. MCell can handle highly detailed 3D geometries, allowing for the simulation of biological environments with realistic spatial constraints. To facilitate the visualisation of these complex geometries, MCell integrates with CellBlender. CellBlender is an addon for Blender, a widely used open-source 3D modelling and animation software [@gupta2018Spatial].

Reaction-based models typically rely on networks to represent chemical interactions (as discussed in @sec-complexity-systems-biology), but they can become computationally expensive as system complexity grows, especially when handling large numbers of reactions or species. MCell, however, bypasses this limitation by tracking individual molecules and their interactions in real-time, only focusing on the relevant species and states. Its particle-based rule evaluation eliminates the need to construct full or partial reaction networks, both at the start and during the simulation, by concentrating solely on the molecules present, their current states, and the reactions they can participate in at any given moment. This approach, known as network-free simulation, not only reduces computational demands but also allows researchers to explore more complex and realistic biological scenarios without the constraints of predefined reaction networks.

As a spatial particle-based simulator, MCell models molecules as point particles within a 3D space. Each time step of an MCell simulation, particles can move, interact with other particles or surfaces, and undergo bimolecular and/or unimolecular reactions. Briefly, MCell operates as follows: as a volume molecule diffuses, all molecules within a given radius along its trajectory, or at the point of collision on a surface, are considered for a reaction. For surface molecules (in membranes), the molecule first diffuses, and then its neighbours are evaluated for reaction [@fig-mcell-works]. Moreover, MCell allows defining arbitrary geometry, and complex models such as a 180μm3 3DEM reconstruction of hippocampal neuropil have been used to construct a geometrically-precise simulation of 100s of neuronal synapses at once (Keller et al., 2015). A detailed description of mathematical foundations of MCell’s algorithms can be found here [@bartol2000Monte; @kerr2008FAST]

![MCell simplified schematic to understand how particle collision works. At time zero, molecules are released into a 3D space (represented by a blue circle), and they begin to diffuse according to specified diffusion coefficients. Over time, if molecules A and B collide, they are considered for a reaction, in this case, where A and B combine to form molecule C. ](30-modelling-figures\mcell_works.PNG){#fig-mcell-works width=85%}

MCell software employs stochastic Monte Carlo algorithms, which rely on random sampling to simulate the behaviour of molecules. This stochastic approach makes MCell particularly effective for capturing the inherent randomness found in biological systems, previously discussed in @sec-odessa. MCell can model the diffusion of molecules within cellular compartments and the probabilistic interactions between proteins, making it a powerful tool for studying phenomena where small fluctuations can have significant impacts, such as in the postsynaptic signalling pathways studied in this thesis. 

MCell (version 4, the one used here) also provides a versatile Python interface. This recently implemented Python interface allows for greater ease in scripting and model construction, making it more user-friendly and adaptable for researchers [REFERENCE]. Additionally, the newly created BioNetGen Library for python (pyBNG) allows direct loading and parsing of a BNG model into MCell. This allows for the creation of models that capture multimeric structures, site-specific binding properties, and the dynamic interactions of proteins over time and space [@@husar2022MCell4]. Next, we provide a more detailed discussion emphasizing how this approach supports the goals of this PhD project.

#### MCell and BioNetGen integration {.unnumbered}

The main components of MCell4, which enable interaction between various libraries and engines, include: a PyMCell library which offers a Python interface and contains classes to manage the model representation. The MCell4 engine, responsible for executing simulation algorithms, and a pyBNG library, which provides methods for resolving BioNetGen reactions. It also includes a MDL (Model Description Language) parser that ensures backward compatibility with MCell3 (not shown here) [@fig-mcellparsing].

![Figure modified from @husar2022MCell4. The MCell4 (version used in this PhD) components that allow for integration between different libraries and engines. PyMCell Library allows for python interface for model representation. The MCell4 engine implements simulation algorithms through a scheduler that keeps track of events to be run in each iteration. The pyBNG library provides methods to resolve BioNetGen reactions using python scripts.](30-modelling-figures\mcellparsing.PNG){#fig-mcellparsing width=70%}

A model can be defined using BNGL, where molecular interactions are specified through detailed rules and reactions. This model can then be parsed using the pyBNG library, allowing Python scripts to handle the BNGL files and initiate simulations with MCell. Python scripts play a crucial role in this workflow. They are not only responsible for parsing BNGL files but also for defining the cellular geometry and setting parameters required for MCell simulations (see code workflow here - REFERENCE). Once the simulation is executed in MCell, the output is generated in the form of data files, containing time-series data on molecular concentrations and dynamics as dictated by the initial reactions [see @fig-input-output].

To illustrate the integration between MCell and BioNetGen using python scripts, see @fig-bngl-workflow, where a BNGL file defines species, reaction rules, molecule released and compartments. These elements are then imported into MCell, as shown in @fig-mcell-workflow, through a python script that "calls" the BNGL file to run it through MCell, using the mechanisms described in @fig-mcellparsing. 

::: {#fig-workflow layout-ncol=1}

![An example BNGL model file titled "ABC.bngl" includes initial parameters, defined compartments, and species within those compartments. It also contains a simple reaction rule, where A + B produces C, and the reverse reaction is also possible.](30-modelling-figures\mcell4BNGLworkflow.PNG){#fig-bngl-workflow width=70%}

![The Python file run_model.py imports mcell as a module via the PyMCell library. This script loads the BNGL file shown in (a), where the entire BNGL file is read, the model is initialised and run for 10 iterations. Alternatively, specific parts of the BNGL file, such as reaction rules or compartment and molecule release information, can be loaded. Additionally, BNGL compartments can be replaced with actual 3D geometry.](30-modelling-figures\mcell4BNGLworkflow2.PNG){#fig-mcell-workflow width=70%}

BioNetGen model files (a) can be integrated into MCell through Python scripts (b), facilitated by the inbuilt pyBNG and pyMCell libraries.
:::

Standard BNGL files, such as the one shown in @fig-bngl-workflow, are compatible with tools like BioNetGen itself, which enables rapid validation of a reaction network using BioNetGen’s ODE solver or other analysis tools. This method allows the model to be directly compared with spatial simulation results in MCell4, eliminating the need of creating multiple versions of the same model. This integration of tools not only provides a more accurate depiction of cellular dynamics but also enhances reproducibility, as the Python scripting enables the simulations to be replicated and adjusted easily. While this overview provides a general understanding of the model's inputs and outputs, specific methodological details will be elaborated in the Methods section.

For a simple but more comprehensive model that incorporates detailed compartments and cell geometry, you can access, download and execute the code here [TEST_ABC](https://github.com/Susana465/test_ABC). This repository includes a set of models where spatial features are taken into consideration, and a thorough README is provided with step-by-step instructions that are beyond the scope of this introduction chapter. 

Alltogether, the integration of MCell and BioNetGen provides a robust framework for addressing key challenges in this PhD project. BioNetGen's rule-based modelling approach effectively addresses the specification problem arising from the combinatorial complexity of the biochemical systems under study. By enabling the concise representation of CaMKII as a multimeric molecule and its interactions with other multimeric proteins, BioNetGen simplifies the modelling process. MCell complements this by allowing these molecular interactions to be modelled within spatially defined geometries. These geometries can be visualised through CellBlender, enabling clear representation of the modelled systems [IS THIS STAYING IN OR NOT]. Additionally, the data output from simulations can be further analysed and interpreted to explore key behavioural patterns, quantify interaction dynamics, and assess model accuracy, providing a deeper understanding of the system's behaviour. The particle-based, network-free simulation capabilities of MCell are critically helpful for addressing the computational challenges associated with the combinatorial complexity of CaMKII. Moreover, MCell enables the stochastic modelling of molecules within defined environments, such as dendritic spines, providing advantages that are key for studying the stochastic dynamics relevant to this research.

#### Birds eye view of model inputs and outputs {.unnumbered}
To provide an overview of the implementation of model files (@fig-workflow) and to clarify the overall process of what goes into a model and what is generated, see a simplified illustration in (@fig-input-output). The inputs to the model are the model files, which are written in the chosen modelling language, such as Python or BioNetGen. These input files include the reaction rules, the definitions of molecular species, and other parameters that define the model. The molecular interactions defined in these models are executed and simulated within a 3D environment, and an output is produced, typically displaying changes in molecular concentrations over time.

In the context of this PhD, this involves constructing a model that defines CaMKII and NMDAR interactions, along with other relevant molecules (detailed in the METHODS section). These interactions are simulated within a postsynaptic dendritic volume, and the resulting data captures the variations in molecular concentrations over time. Capturing the variations in molecular concentrations over time allows for a detailed analysis of the kinetics of these interactions, providing insights into the rates and mechanisms of biochemical reactions at a level of precision that is challenging to achieve experimentally. This information can then reveal how specific molecular dynamics contribute to larger-scale cellular behaviours like LTP for understanding how learning and memory work. Additionally, understanding the chemical properties of the reactions involved, including binding affinities, reaction rates, and diffusion characteristics, can aid in predicting how changes at the molecular level may impact the overall function of the synapse. 

::: {#fig-workflow layout-ncol=1}

![](30-modelling-figures\inputoutput.PNG){#fig-input-output}

![](30-modelling-figures\inputoutput2.PNG){#fig-input-output2}

(a) What goes in and what comes out when modelling. An overview of input and output workflow of models in this project. Input files define the model, which is run to simulate difussion of molecules in a 3D space, to give an output of molecular concetrations across time. (b) There are alternative workflows for modelling and visualisation. BioNetGen uses .BNGL files to define biochemical reactions. MCell uses .py files. Outputs from either path can be visualised as graphs of molecular concentration over time or as 3D spatial distributions in CellBlender. The arrows emphasise the flexible flow between tools, showing how BioNetGen and MCell outputs can be independently or jointly visualised to provide both temporal and spatial insights into molecular dynamics.
:::

link to reproducibility , validation process and if i dont use here i can reuse in methods.

## Agent based modelling with BioDynaMo for visualising multi-dimensional simulations

With MCell and BioNetGen, we are able to model a CaMKII dodecamer in a 3D environment, but what about the dynamic reshaping of a neuron which happens during LTP discussed in @SECTIONLINK? CaMKII interacts with the actin cytoskeleton, which is responsible for the shape and size of the dendritic spine [@okamoto2007Role]. To study this dynamic reshaping, BioDynaMo (Biology Dynamics Modeller) offers a solution, as it can simulate changes in cell shape, such as dendritic growth (@fig-biodynamoactin).

Similar to MCell, BioDynaMo is an agent-based modelling tool that enables the simulation of 3D biophysical molecular interactions [@breitwieser2021BioDynaMo]. However, unlike MCell, BioDynaMo supports dynamic geometry modelling, allowing geometries to change in size or shape during a simulation. In MCell, geometries are static and predefined at the start of the simulation, and they remain fixed throughout the modelling process. To simulate dynamic changes, such as a geometry growing, shrinking, or morphing over time, we can use BioDynaMo. This way, MCell and BioNetGen allow modelling multmeric protein dynamics with a rule based approach (which BioDynaMo does not offer) and BioDynaMo allows for dynamic geometry modelling. 

![Actin filaments modelled in BioDynaMo showing mushroom-like shaped polymerization on a dendritic spine. Data not published, Trajlinek A., 2022](30-modelling-figures\BioDynaMo-actin.PNG){#fig-biodynamoactin width=50%}

We have seen how BioNetGen and MCell can be integrated and used together via the Python API. But how can we extend this integration to include BioDynaMo? The key lies in SBML integration. BioDynaMo supports the Systems Biology Markup Language (SBML), a widely recognised standard for representing biological models. SBML enables the translation and integration of models written in different formats, allowing them to be executed across a range of software platforms [@hucka2003Systems; @keating2020SBML]. As a well-established standard, SBML is frequently used to describe chemical reaction networks, such as those involved in metabolic processes or cell signalling pathways.

Critically, BioNetGen can export models written in BNGL to the SBML format. This export capability allows for the conversion of rule-based BNGL models into a format that is compatible with SBML-supporting tools like BioDynaMo. Consequently, this interoperability facilitates the integration of BNGL models with BioDynaMo, harnessing BioNetGen’s strengths in rule-based modelling alongside BioDynaMo’s advanced capabilities for dynamic geometry simulations. 

To demonstrate the possibility of using BioDynaMo as a tool to simulate neuronal growth through formation of an actin cytoskeleton inside dendritic spines, I helped with supervision of a project that carried out by Adrian Trajlinek (Reference not published). The model works with actin filaments represented as molecular agents with cylindrical shapes [@fig-biodynamoactin]. These can then be modelled to support tree-like structures that can branch out, sever, polymerize and depolymerize in similar manners to the biological equivalent. Overall, this project provided proof of concept that actin dynamics can be modelled in BioDynaMo, and further research can be developed from here onwards. Tools like MCell+CellBlender, BioDynaMo and RuleBender provide an exciting prospect of modelling how the morphology of dendritic spines changes during synaptic plasticity. Together, they provide tools to build a general-purpose platform for large-scale biological simulations.

EXTRA
REPRODUCIBILITY LINK?
Similarly, another aspect that is important when creating computer models, is writing code in clear, human-readable formats, wherever possible, and to employ version control systems. - this sentence feels vague needs more*

These practices facilitate efficient tracking of changes, support collaborative code reviews, and enable team members to inspect and discuss updates to the model effectively. Together, all these steps are not so different to wet-lab work, and draw significant parallels. Both require meticulous attention to the assumptions made about the system under study, as well as careful planning and execution of experimental design. The process demands rigorous consideration to ensure that models, like wet-lab experiments, are both robust and reliable. Validation is a key step when considering reproducibility and the future of a project, therefore it relates to REPRO-CHAPTER. 


